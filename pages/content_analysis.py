import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import jieba
import jieba.analyse
from collections import Counter
import re
import sys
import os
import platform
from pathlib import Path
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
from textstat import flesch_reading_ease
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils.visualizer import UserBehaviorVisualizer, create_dashboard_metrics, display_metrics_cards
from utils.cache_manager import cache_data
from config.settings import get_config

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å†…å®¹è¡Œä¸ºåˆ†æ",
    page_icon="ğŸ“",
    layout="wide"
)

class ContentAnalyzer:
    """å†…å®¹è¡Œä¸ºåˆ†æå™¨"""
    
    def __init__(self):
        self.visualizer = UserBehaviorVisualizer()
        self.viz_config = get_config('viz')
        
        # åˆå§‹åŒ–jieba
        jieba.initialize()
        
        # æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨
        self.stop_words = set([
            # åŸºç¡€åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª',
            # ç–‘é—®è¯
            'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å“ªä¸ª', 'å¤šå°‘', 'å‡ ä¸ª', 'æ€æ ·', 'å¦‚ä½•', 'å“ªäº›', 'è°', 'ä½•æ—¶', 'ä½•åœ°',
            # æ•°é‡è¯
            'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'ä¸€äº›', 'å¾ˆå¤š', 'è®¸å¤š', 'å¤§é‡', 'å°‘é‡', 'å…¨éƒ¨', 'éƒ¨åˆ†', 'æ‰€æœ‰',
            # æƒ…æ€è¯
            'å¯ä»¥', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'å¿…é¡»', 'éœ€è¦', 'æƒ³è¦', 'å¸Œæœ›', 'æ„¿æ„', 'æ‰“ç®—', 'å‡†å¤‡',
            # æ—¶é—´è¯
            'å·²ç»', 'æ­£åœ¨', 'å°†è¦', 'æ›¾ç»', 'ä»æ¥', 'æ€»æ˜¯', 'ç»å¸¸', 'æœ‰æ—¶', 'å¶å°”', 'ä»ä¸',
            # è¿æ¥è¯
            'è¿˜æ˜¯', 'æˆ–è€…', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'æ¥ç€', 'äºæ˜¯', 'å› æ­¤', 'ç„¶è€Œ', 'ä¸è¿‡', 'è€Œä¸”', 'å¹¶ä¸”', 'ä»¥åŠ',
            # æ—¶é—´è¡¨è¾¾
            'ç°åœ¨', 'ä»¥å', 'ä»¥å‰', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'å‰å¤©', 'åå¤©', 'æœ€è¿‘', 'åˆšæ‰', 'é©¬ä¸Š', 'ç«‹åˆ»', 'çªç„¶',
            # ç¨‹åº¦è¯
            'éå¸¸', 'ç‰¹åˆ«', 'ååˆ†', 'ç›¸å½“', 'æ¯”è¾ƒ', 'æ›´åŠ ', 'æœ€', 'æå…¶', 'æ ¼å¤–', 'å°¤å…¶', 'ç‰¹åˆ«æ˜¯',
            # æ–¹ä½è¯
            'è¿™é‡Œ', 'é‚£é‡Œ', 'å“ªé‡Œ', 'åˆ°å¤„', 'å¤„å¤„', 'å„å¤„', 'æŸå¤„', 'åˆ«å¤„', 'æ­¤å¤„', 'å½¼å¤„',
            # ä»£è¯
            'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'å¤§å®¶', 'åˆ«äºº', 'å…¶ä»–', 'å¦å¤–', 'å„è‡ª', 'å½¼æ­¤',
            # æ ‡ç‚¹å’Œç¬¦å·
            'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', '"', "'", 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹',
            # ç½‘ç»œç”¨è¯­
            'http', 'https', 'www', 'com', 'cn', 'org', 'net', 'html', 'php', 'asp',
            # æ— æ„ä¹‰è¯æ±‡
            'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å“ˆ', 'å‘µ', 'å˜¿', 'å“Ÿ', 'å’¦', 'å“‡', 'å”‰', 'é¢', 'å‘ƒ', 'å—¯å—¯', 'å“ˆå“ˆ',
            # å¸¸è§åŠ¨è¯
            'åš', 'æ', 'å¼„', 'æ¥', 'èµ°', 'è·‘', 'å', 'ç«™', 'èºº', 'ç¡', 'åƒ', 'å–', 'ä¹°', 'å–', 'ç»™', 'æ‹¿', 'æ”¾',
            # å¸¸è§å½¢å®¹è¯
            'å¤§', 'å°', 'é«˜', 'ä½', 'é•¿', 'çŸ­', 'æ–°', 'æ—§', 'å¤š', 'å°‘', 'å¿«', 'æ…¢', 'æ—©', 'æ™š', 'è¿œ', 'è¿‘',
            # å…¶ä»–å¸¸è§è¯
            'ä¸œè¥¿', 'äº‹æƒ…', 'é—®é¢˜', 'æ–¹é¢', 'æƒ…å†µ', 'æ—¶å€™', 'åœ°æ–¹', 'æ–¹å¼', 'æ–¹æ³•', 'ç»“æœ', 'åŸå› ', 'ç›®çš„', 'æ„æ€', 'å†…å®¹', 'æ–¹å‘'
        ])
    
    @cache_data(ttl=1800)
    def analyze_text_content(self, df: pd.DataFrame, text_column: str = 'å¾®åšæ–‡æœ¬') -> dict:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        analysis = {}
        
        if text_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼å’Œå¼‚å¸¸å†…å®¹
        df_text = df[df[text_column].notna()].copy()
        if df_text.empty:
            return analysis
        
        # æ•°æ®æ¸…æ´—
        texts = df_text[text_column].astype(str)
        
        # è¿‡æ»¤å¼‚å¸¸å†…å®¹
        import re
        cleaned_texts = []
        for text in texts:
            text = text.strip()
            # è¿‡æ»¤ç©ºç™½å†…å®¹
            if not text or text.isspace():
                continue
            # è¿‡æ»¤åªæœ‰ç©ºæ ¼çš„å†…å®¹
            if len(text.replace(' ', '').replace('\t', '').replace('\n', '')) == 0:
                continue
            # è¿‡æ»¤è¿‡çŸ­å†…å®¹ï¼ˆå°‘äº2ä¸ªæœ‰æ•ˆå­—ç¬¦ï¼‰
            if len(re.sub(r'\s+', '', text)) < 2:
                continue
            # è¿‡æ»¤åŒ…å«å¤§é‡æ•°å­—æˆ–è‹±æ–‡çš„å¼‚å¸¸å†…å®¹
            if re.search(r'[0-9]{8,}', text) or re.search(r'[A-Za-z]{15,}', text):
                continue
            cleaned_texts.append(text)
        
        if not cleaned_texts:
            return analysis
            
        # å»é‡
        texts = pd.Series(cleaned_texts).drop_duplicates()
        
        # åŸºç¡€ç»Ÿè®¡
        analysis['basic_stats'] = {
            'total_posts': len(texts),
            'avg_length': texts.str.len().mean(),
            'median_length': texts.str.len().median(),
            'max_length': texts.str.len().max(),
            'min_length': texts.str.len().min(),
            'std_length': texts.str.len().std()
        }
        
        # é•¿åº¦åˆ†å¸ƒ
        length_bins = [0, 50, 100, 200, 500, float('inf')]
        length_labels = ['çŸ­æ–‡æœ¬(â‰¤50)', 'ä¸­çŸ­æ–‡æœ¬(51-100)', 'ä¸­ç­‰æ–‡æœ¬(101-200)', 'é•¿æ–‡æœ¬(201-500)', 'è¶…é•¿æ–‡æœ¬(>500)']
        length_distribution = pd.cut(texts.str.len(), bins=length_bins, labels=length_labels, right=False)
        analysis['length_distribution'] = length_distribution.value_counts().to_dict()
        
        # æå–å…³é”®è¯
        all_text = ' '.join(texts)
        keywords = jieba.analyse.extract_tags(all_text, topK=50, withWeight=True)
        analysis['keywords'] = [(word, weight) for word, weight in keywords if word not in self.stop_words]
        
        # è¯é¢‘ç»Ÿè®¡ - å¢å¼ºç‰ˆ
        words = []
        for text in texts:
            # åˆ†è¯å¹¶è¿‡æ»¤
            text_words = jieba.cut(text)
            filtered_words = [
                word.strip() for word in text_words 
                if len(word.strip()) >= 2  # è‡³å°‘2ä¸ªå­—ç¬¦
                and word.strip() not in self.stop_words  # ä¸åœ¨åœç”¨è¯ä¸­
                and not word.strip().isdigit()  # ä¸æ˜¯çº¯æ•°å­—
                and not word.strip().isspace()  # ä¸æ˜¯ç©ºç™½å­—ç¬¦
                and word.strip()  # ä¸ä¸ºç©º
            ]
            words.extend(filtered_words)
        
        word_freq = Counter(words)
        # è¿‡æ»¤ä½é¢‘è¯ï¼ˆå‡ºç°æ¬¡æ•°å°‘äº2æ¬¡çš„è¯ï¼‰
        filtered_freq = {word: freq for word, freq in word_freq.items() if freq >= 2}
        analysis['word_frequency'] = dict(Counter(filtered_freq).most_common(50))
        
        # æƒ…æ„Ÿè¯åˆ†æï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        positive_words = ['å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'å¼€å¿ƒ', 'å¿«ä¹', 'æ»¡æ„', 'ä¼˜ç§€', 'å®Œç¾', 'ç¾å¥½', 'å¹¸ç¦', 'æˆåŠŸ']
        negative_words = ['å·®', 'å', 'çƒ‚', 'è®¨åŒ', 'éš¾è¿‡', 'å¤±æœ›', 'ç³Ÿç³•', 'ç—›è‹¦', 'å¤±è´¥', 'é—®é¢˜', 'é”™è¯¯']
        
        positive_count = sum(text.count(word) for text in texts for word in positive_words)
        negative_count = sum(text.count(word) for text in texts for word in negative_words)
        
        analysis['sentiment_simple'] = {
            'positive_mentions': positive_count,
            'negative_mentions': negative_count,
            'sentiment_ratio': positive_count / (positive_count + negative_count + 1)
        }
        
        # ç‰¹æ®Šå­—ç¬¦å’Œè¡¨æƒ…åˆ†æ
        emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ‡€-ğŸ‡¿]+', re.UNICODE)
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        mention_pattern = re.compile(r'@[\w\u4e00-\u9fff]+')
        hashtag_pattern = re.compile(r'#[^#]+#')
        
        emoji_count = sum(len(emoji_pattern.findall(text)) for text in texts)
        url_count = sum(len(url_pattern.findall(text)) for text in texts)
        mention_count = sum(len(mention_pattern.findall(text)) for text in texts)
        hashtag_count = sum(len(hashtag_pattern.findall(text)) for text in texts)
        
        analysis['special_elements'] = {
            'emoji_count': emoji_count,
            'url_count': url_count,
            'mention_count': mention_count,
            'hashtag_count': hashtag_count,
            'avg_emoji_per_post': emoji_count / len(texts),
            'posts_with_emoji': sum(1 for text in texts if emoji_pattern.search(text)),
            'posts_with_url': sum(1 for text in texts if url_pattern.search(text)),
            'posts_with_mention': sum(1 for text in texts if mention_pattern.search(text)),
            'posts_with_hashtag': sum(1 for text in texts if hashtag_pattern.search(text))
        }
        
        return analysis
    
    @cache_data(ttl=1800)
    def analyze_posting_sources(self, df: pd.DataFrame, source_column: str = 'å‘å¸ƒæ¥æº') -> dict:
        """åˆ†æå‘å¸ƒæ¥æº"""
        analysis = {}
        
        if source_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼
        df_source = df[df[source_column].notna()].copy()
        if df_source.empty:
            return analysis
        
        sources = df_source[source_column]
        
        # æ¥æºåˆ†å¸ƒ
        source_counts = sources.value_counts()
        analysis['source_distribution'] = source_counts.to_dict()
        
        # æ¥æºç±»å‹åˆ†ç±»
        mobile_keywords = ['iPhone', 'Android', 'æ‰‹æœº', 'ç§»åŠ¨', 'mobile', 'iOS']
        web_keywords = ['ç½‘é¡µ', 'web', 'æµè§ˆå™¨', 'PC', 'ç”µè„‘']
        app_keywords = ['å®¢æˆ·ç«¯', 'APP', 'åº”ç”¨']
        
        mobile_count = 0
        web_count = 0
        app_count = 0
        other_count = 0
        
        for source in sources:
            source_lower = str(source).lower()
            if any(keyword.lower() in source_lower for keyword in mobile_keywords):
                mobile_count += 1
            elif any(keyword.lower() in source_lower for keyword in web_keywords):
                web_count += 1
            elif any(keyword.lower() in source_lower for keyword in app_keywords):
                app_count += 1
            else:
                other_count += 1
        
        analysis['source_categories'] = {
            'mobile': mobile_count,
            'web': web_count,
            'app': app_count,
            'other': other_count
        }
        
        # ä¸»è¦æ¥æºç»Ÿè®¡
        total_posts = len(sources)
        analysis['source_stats'] = {
            'total_sources': len(source_counts),
            'most_popular_source': source_counts.index[0] if len(source_counts) > 0 else None,
            'most_popular_count': source_counts.iloc[0] if len(source_counts) > 0 else 0,
            'most_popular_ratio': source_counts.iloc[0] / total_posts if len(source_counts) > 0 else 0,
            'source_diversity': len(source_counts) / total_posts  # æ¥æºå¤šæ ·æ€§
        }
        
        return analysis
    
    @cache_data(ttl=1800)
    def analyze_content_topics(self, df: pd.DataFrame, text_column: str = 'å¾®åšæ–‡æœ¬') -> dict:
        """åˆ†æå†…å®¹ä¸»é¢˜"""
        analysis = {}
        
        if text_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼
        df_text = df[df[text_column].notna()].copy()
        if df_text.empty:
            return analysis
        
        texts = df_text[text_column].astype(str)
        
        # ä¸»é¢˜å…³é”®è¯åˆ†ç±»ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        topic_keywords = {
            'ç”Ÿæ´»æ—¥å¸¸': ['ç”Ÿæ´»', 'æ—¥å¸¸', 'åƒé¥­', 'ç¡è§‰', 'å·¥ä½œ', 'å­¦ä¹ ', 'å®¶', 'æœ‹å‹', 'å®¶äºº'],
            'æƒ…æ„Ÿè¡¨è¾¾': ['çˆ±', 'å–œæ¬¢', 'è®¨åŒ', 'å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”', 'æ„ŸåŠ¨', 'æƒ³å¿µ', 'æ€å¿µ'],
            'å¨±ä¹ä¼‘é—²': ['ç”µå½±', 'éŸ³ä¹', 'æ¸¸æˆ', 'æ—…æ¸¸', 'è´­ç‰©', 'ç¾é£Ÿ', 'è¿åŠ¨', 'å¥èº«'],
            'ç¤¾ä¼šçƒ­ç‚¹': ['æ–°é—»', 'æ”¿æ²»', 'ç»æµ', 'ç¤¾ä¼š', 'çƒ­ç‚¹', 'äº‹ä»¶', 'è®¨è®º', 'è§‚ç‚¹'],
            'ç§‘æŠ€æ•°ç ': ['ç§‘æŠ€', 'æ‰‹æœº', 'ç”µè„‘', 'è½¯ä»¶', 'ç½‘ç»œ', 'äº’è”ç½‘', 'AI', 'æŠ€æœ¯'],
            'å¥åº·å…»ç”Ÿ': ['å¥åº·', 'å…»ç”Ÿ', 'åŒ»ç–—', 'é”»ç‚¼', 'é¥®é£Ÿ', 'è¥å…»', 'ä¿å¥', 'èº«ä½“'],
            'æ•™è‚²å­¦ä¹ ': ['å­¦ä¹ ', 'æ•™è‚²', 'çŸ¥è¯†', 'æŠ€èƒ½', 'è¯¾ç¨‹', 'è€ƒè¯•', 'è¯»ä¹¦', 'æˆé•¿'],
            'èŒåœºå·¥ä½œ': ['å·¥ä½œ', 'èŒåœº', 'åŒäº‹', 'è€æ¿', 'é¡¹ç›®', 'ä¼šè®®', 'åŠ ç­', 'å‡èŒ']
        }
        
        topic_counts = {topic: 0 for topic in topic_keywords.keys()}
        topic_posts = {topic: [] for topic in topic_keywords.keys()}
        
        for idx, text in enumerate(texts):
            text_lower = text.lower()
            for topic, keywords in topic_keywords.items():
                if any(keyword in text_lower for keyword in keywords):
                    topic_counts[topic] += 1
                    topic_posts[topic].append(idx)
        
        analysis['topic_distribution'] = topic_counts
        analysis['topic_posts'] = topic_posts
        
        # ä¸»é¢˜å¤šæ ·æ€§
        total_classified = sum(topic_counts.values())
        unclassified = len(texts) - total_classified
        
        analysis['topic_stats'] = {
            'total_classified': total_classified,
            'unclassified': unclassified,
            'classification_rate': total_classified / len(texts),
            'most_popular_topic': max(topic_counts, key=topic_counts.get) if total_classified > 0 else None,
            'topic_diversity': len([count for count in topic_counts.values() if count > 0])
        }
        
        return analysis
    
    def is_cloud_environment(self):
        """æ£€æµ‹æ˜¯å¦åœ¨äº‘ç¯å¢ƒä¸­è¿è¡Œ"""
        # æ£€æµ‹å¸¸è§çš„äº‘ç¯å¢ƒæ ‡è¯†
        cloud_indicators = [
            'STREAMLIT_SHARING',  # Streamlit Cloud
            'HEROKU',             # Heroku
            'VERCEL',             # Vercel
            'NETLIFY',            # Netlify
            'AWS_LAMBDA_FUNCTION_NAME',  # AWS Lambda
            'GOOGLE_CLOUD_PROJECT',      # Google Cloud
        ]
        
        for indicator in cloud_indicators:
            if os.environ.get(indicator):
                return True
        
        # æ£€æµ‹æ˜¯å¦åœ¨å®¹å™¨ç¯å¢ƒä¸­
        if os.path.exists('/.dockerenv'):
            return True
            
        # æ£€æµ‹Streamlit Cloudç‰¹æœ‰è·¯å¾„
        if '/mount/src' in os.getcwd():
            return True
        
        # æ£€æµ‹æ˜¯å¦ä¸ºWindowsæœ¬åœ°ç¯å¢ƒ
        if platform.system() == 'Windows':
            # Windowsæœ¬åœ°ç¯å¢ƒé€šå¸¸æœ‰è¿™äº›ç‰¹å¾
            if os.path.exists('C:/Windows'):
                return False
                
        return False
    
    def detect_chinese_font(self):
        """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„ä¸­æ–‡å­—ä½“è·¯å¾„"""
        # äº‘ç¯å¢ƒä¹Ÿå°è¯•æ£€æµ‹å­—ä½“
        is_cloud = self.is_cloud_environment()
        if is_cloud:
            # äº‘ç¯å¢ƒå­—ä½“æ£€æµ‹
            return self._detect_cloud_chinese_font()
    
    def _detect_cloud_chinese_font(self):
        """ä¸“é—¨ä¸ºäº‘ç¯å¢ƒæ£€æµ‹ä¸­æ–‡å­—ä½“"""
        try:
            import matplotlib.font_manager as fm
            
            # ä¼˜å…ˆæŸ¥æ‰¾Notoå­—ä½“ï¼ˆGoogleå¼€æºï¼Œäº‘ç¯å¢ƒå¸¸è§ï¼‰
            noto_fonts = []
            for font in fm.fontManager.ttflist:
                font_name_lower = font.name.lower()
                if 'noto' in font_name_lower and ('cjk' in font_name_lower or 'sans' in font_name_lower):
                    noto_fonts.append((font.name, font.fname))
            
            if noto_fonts:
                font_name, font_path = noto_fonts[0]
                st.info(f"â˜ï¸ äº‘ç¯å¢ƒæ¨¡å¼ï¼šæ‰¾åˆ°Notoå­—ä½“ {font_name}")
                return font_path
            
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ä¸­æ–‡å­—ä½“
            chinese_keywords = ['simhei', 'simsun', 'yahei', 'kai', 'song', 'hei', 'chinese', 'cjk', 'han', 'source', 'adobe']
            chinese_fonts = []
            for font in fm.fontManager.ttflist:
                font_name_lower = font.name.lower()
                if any(keyword in font_name_lower for keyword in chinese_keywords):
                    chinese_fonts.append((font.name, font.fname))
            
            if chinese_fonts:
                font_name, font_path = chinese_fonts[0]
                st.info(f"â˜ï¸ äº‘ç¯å¢ƒæ¨¡å¼ï¼šæ‰¾åˆ°ä¸­æ–‡å­—ä½“ {font_name}")
                return font_path
            
            # æŸ¥æ‰¾DejaVuå­—ä½“ä½œä¸ºå¤‡é€‰
            dejavu_fonts = []
            for font in fm.fontManager.ttflist:
                if 'dejavu' in font.name.lower() and 'sans' in font.name.lower():
                    dejavu_fonts.append((font.name, font.fname))
            
            if dejavu_fonts:
                font_name, font_path = dejavu_fonts[0]
                st.warning(f"â˜ï¸ äº‘ç¯å¢ƒæ¨¡å¼ï¼šæœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ {font_name}ï¼ˆå¯èƒ½æ˜¾ç¤ºæ–¹å—ï¼‰")
                return font_path
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆé€‚å­—ä½“ï¼Œè¿”å›None
            st.warning("â˜ï¸ äº‘ç¯å¢ƒå­—ä½“æ£€æµ‹å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
            return None
            
        except Exception as e:
            st.warning(f"â˜ï¸ äº‘ç¯å¢ƒå­—ä½“æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    def _try_download_cloud_font(self):
         """å°è¯•ä¸ºäº‘ç¯å¢ƒä¸‹è½½ä¸­æ–‡å­—ä½“"""
         try:
             import tempfile
             import urllib.request
             
             # ä½¿ç”¨å¼€æºçš„TTFä¸­æ–‡å­—ä½“ï¼ˆæ›´å…¼å®¹wordcloudï¼‰
             font_urls = [
                 # ä½¿ç”¨GitHubä¸Šçš„å¼€æºä¸­æ–‡å­—ä½“
                 'https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Regular.otf',
                 'https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf'
             ]
             
             # åˆ›å»ºä¸´æ—¶ç›®å½•
             temp_dir = tempfile.mkdtemp()
             
             for i, url in enumerate(font_urls):
                 try:
                     font_extension = url.split('.')[-1]
                     font_path = os.path.join(temp_dir, f'chinese_font_{i}.{font_extension}')
                     
                     # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
                     req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                     with urllib.request.urlopen(req, timeout=30) as response:
                         with open(font_path, 'wb') as f:
                             f.write(response.read())
                     
                     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸‹è½½æˆåŠŸ
                     if os.path.exists(font_path) and os.path.getsize(font_path) > 1000:  # è‡³å°‘1KB
                         return font_path
                         
                 except Exception as download_error:
                     st.warning(f"å­—ä½“ä¸‹è½½å¤±è´¥ {i+1}: {download_error}")
                     continue
             
             return None
             
         except Exception as e:
             st.warning(f"äº‘ç¯å¢ƒå­—ä½“ä¸‹è½½åŠŸèƒ½å¤±è´¥: {e}")
             return None
    
    def detect_chinese_font(self):
        """æ£€æµ‹å¯ç”¨çš„ä¸­æ–‡å­—ä½“"""
        # äº‘ç¯å¢ƒä¼˜å…ˆä½¿ç”¨ä¸“ç”¨æ£€æµ‹
        if self.is_cloud_environment():
            cloud_font = self._detect_cloud_chinese_font()
            if cloud_font:
                return cloud_font
            
            # äº‘ç¯å¢ƒå­—ä½“æ£€æµ‹å¤±è´¥æ—¶çš„è­¦å‘Š
            st.warning("äº‘ç¯å¢ƒå­—ä½“æ£€æµ‹å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
        
        # æœ¬åœ°ç¯å¢ƒå­—ä½“æ£€æµ‹
        # å¸¸è§ä¸­æ–‡å­—ä½“è·¯å¾„
        font_paths = []
        
        # Windowsç³»ç»Ÿå­—ä½“è·¯å¾„
        if platform.system() == 'Windows':
            windows_fonts = [
                'C:/Windows/Fonts/simhei.ttf',  # é»‘ä½“
                'C:/Windows/Fonts/simsun.ttc',  # å®‹ä½“
                'C:/Windows/Fonts/msyh.ttc',    # å¾®è½¯é›…é»‘
                'C:/Windows/Fonts/simkai.ttf',  # æ¥·ä½“
            ]
            font_paths.extend(windows_fonts)
        
        # macOSç³»ç»Ÿå­—ä½“è·¯å¾„
        elif platform.system() == 'Darwin':
            mac_fonts = [
                '/System/Library/Fonts/PingFang.ttc',
                '/System/Library/Fonts/Hiragino Sans GB.ttc',
                '/Library/Fonts/Arial Unicode MS.ttf',
                '/System/Library/Fonts/STHeiti Light.ttc',
            ]
            font_paths.extend(mac_fonts)
        
        # Linuxç³»ç»Ÿå­—ä½“è·¯å¾„
        else:
            linux_fonts = [
                '/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf',
                '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',
                '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc',
                '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
                '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
            ]
            font_paths.extend(linux_fonts)
        
        # æ£€æŸ¥å­—ä½“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for font_path in font_paths:
            if os.path.exists(font_path):
                return font_path
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼Œè¿”å›None
        return None
    
    def create_wordcloud(self, word_freq: dict, max_words: int = 100) -> plt.Figure:
        """åˆ›å»ºè¯äº‘å›¾"""
        if not word_freq:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, 'æš‚æ— æ•°æ®', ha='center', va='center', fontsize=20)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
        
        # è·å–ç”¨æˆ·å­—ä½“é…ç½®
        font_config = st.session_state.get('font_config', {
            'selected_font': 'SimHei',
            'font_size': 12
        })
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ
        try:
            # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å­—ä½“
            selected_font = font_config.get('selected_font', 'SimHei')
            plt.rcParams['font.sans-serif'] = [selected_font, 'SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            plt.rcParams['font.size'] = font_config.get('font_size', 12)
        except Exception as e:
            st.warning(f"å­—ä½“è®¾ç½®è­¦å‘Š: {e}")
        
        # æ£€æµ‹è¿è¡Œç¯å¢ƒå’Œå­—ä½“
        is_cloud = self.is_cloud_environment()
        font_path = self.detect_chinese_font()
        
        # åˆ›å»ºè¯äº‘
        try:
            # äº‘ç¯å¢ƒä¼˜åŒ–é…ç½®
            wordcloud_config = {
                'width': 800,
                'height': 400,
                'background_color': 'white',
                'max_words': max_words,
                'colormap': 'viridis',
                'prefer_horizontal': 0.9,
                'relative_scaling': 0.5,
                'collocations': False,
                'mode': 'RGBA'
            }
            
            # æ ¹æ®ç¯å¢ƒè°ƒæ•´é…ç½®
            if is_cloud:
                # äº‘ç¯å¢ƒä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®
                wordcloud_config.update({
                    'max_font_size': 80,
                    'min_font_size': 10,
                    'font_step': 2
                })
                
                # äº‘ç¯å¢ƒå­—ä½“é…ç½® - ç›´æ¥ä½¿ç”¨æ£€æµ‹ç»“æœ
                if font_path:
                    wordcloud_config['font_path'] = font_path
                else:
                    # å¦‚æœå­—ä½“æ£€æµ‹å¤±è´¥ï¼Œå°è¯•ä¸‹è½½å­—ä½“
                    cloud_font_path = self._try_download_cloud_font()
                    if cloud_font_path:
                        wordcloud_config['font_path'] = cloud_font_path
                        st.success("â˜ï¸ äº‘ç¯å¢ƒæ¨¡å¼ï¼šæˆåŠŸä¸‹è½½ä¸­æ–‡å­—ä½“")
                    else:
                        # ä½¿ç”¨ç‰¹æ®Šçš„wordcloudé…ç½®æ¥å¤„ç†Unicode
                        wordcloud_config.update({
                             'font_step': 1,
                             'max_font_size': 60,
                             'min_font_size': 12,
                             'prefer_horizontal': 1.0,  # å¼ºåˆ¶æ°´å¹³æ˜¾ç¤º
                             'relative_scaling': 0.3,
                             'mode': 'RGB'  # æ”¹ç”¨RGBæ¨¡å¼
                        })
                        st.warning("â˜ï¸ äº‘ç¯å¢ƒæ¨¡å¼ï¼šä½¿ç”¨ä¼˜åŒ–é…ç½®ï¼Œå¦‚ä»æ˜¾ç¤ºæ–¹å—è¯·è”ç³»ç®¡ç†å‘˜")
                        
                        # å°è¯•è®¾ç½®matplotlibçš„å­—ä½“å›é€€
                        try:
                            plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
                            plt.rcParams['axes.unicode_minus'] = False
                        except:
                            pass
            else:
                # æœ¬åœ°ç¯å¢ƒå¯ä»¥ä½¿ç”¨æ›´ä¸°å¯Œçš„é…ç½®
                base_font_size = font_config.get('font_size', 12)
                wordcloud_config.update({
                    'font_step': 1,
                    'max_font_size': min(100, base_font_size * 8),
                    'min_font_size': max(8, base_font_size // 2)
                })
                
                # å°è¯•ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å­—ä½“
                selected_font = font_config.get('selected_font', 'SimHei')
                
                # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å­—ä½“å¯¹åº”çš„å­—ä½“æ–‡ä»¶
                font_used = False
                
                # Windowsç³»ç»Ÿå­—ä½“æ˜ å°„
                if platform.system() == 'Windows':
                    font_mapping = {
                        'SimHei': 'C:/Windows/Fonts/simhei.ttf',
                        'SimSun': 'C:/Windows/Fonts/simsun.ttc',
                        'Microsoft YaHei': 'C:/Windows/Fonts/msyh.ttc',
                        'KaiTi': 'C:/Windows/Fonts/simkai.ttf'
                    }
                    
                    if selected_font in font_mapping:
                        font_file = font_mapping[selected_font]
                        if os.path.exists(font_file):
                            wordcloud_config['font_path'] = font_file
                            st.info(f"ğŸ¨ ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å­—ä½“: {selected_font}")
                            font_used = True
                
                # å¦‚æœç”¨æˆ·é€‰æ‹©çš„å­—ä½“ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„å­—ä½“
                if not font_used and font_path:
                    wordcloud_config['font_path'] = font_path
                    st.info(f"ğŸ¨ ä½¿ç”¨æ£€æµ‹åˆ°çš„å­—ä½“æ–‡ä»¶: {os.path.basename(font_path)}")
                    font_used = True
                
                # æœ€åå°è¯•é€šè¿‡matplotlibæŸ¥æ‰¾å­—ä½“
                if not font_used:
                    try:
                        import matplotlib.font_manager as fm
                        # æŸ¥æ‰¾ç”¨æˆ·é€‰æ‹©çš„å­—ä½“
                        font_files = [f for f in fm.fontManager.ttflist if selected_font in f.name]
                        if font_files:
                            wordcloud_config['font_path'] = font_files[0].fname
                            st.info(f"ğŸ¨ é€šè¿‡matplotlibæ‰¾åˆ°å­—ä½“: {selected_font}")
                            font_used = True
                    except Exception as font_error:
                        st.warning(f"å­—ä½“æ£€æµ‹å¤±è´¥: {font_error}")
                
                if not font_used:
                    st.warning(f"ğŸ¨ å­—ä½“ {selected_font} ä¸å¯ç”¨ï¼Œè¯äº‘å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹å—")
                    st.info("ğŸ’¡ å»ºè®®åœ¨ä¾§è¾¹æ é€‰æ‹©å…¶ä»–å¯ç”¨å­—ä½“")
            
            wordcloud = WordCloud(**wordcloud_config).generate_from_frequencies(word_freq)
            
        except Exception as e:
            # å¦‚æœå‡ºç°ä»»ä½•é—®é¢˜ï¼Œä½¿ç”¨æœ€ç®€é…ç½®
            try:
                st.warning(f"è¯äº‘ç”Ÿæˆé‡åˆ°é—®é¢˜ï¼Œå°è¯•ç®€åŒ–é…ç½®: {str(e)}")
                simple_config = {
                    'width': 800,
                    'height': 400,
                    'background_color': 'white',
                    'max_words': max_words,
                    'mode': 'RGBA',
                    'max_font_size': 80,
                    'min_font_size': 10
                }
                
                if font_path:
                    simple_config['font_path'] = font_path
                
                wordcloud = WordCloud(**simple_config).generate_from_frequencies(word_freq)
                
            except Exception as e2:
                st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {str(e2)}")
                st.info("ğŸ’¡ æç¤ºï¼šè¿™å¯èƒ½æ˜¯äº‘ç¯å¢ƒçš„å­—ä½“æˆ–å›¾åƒå¤„ç†é—®é¢˜")
                # æ˜¾ç¤ºå­—ä½“æ£€æµ‹ä¿¡æ¯
                st.info(f"ğŸ” ç³»ç»Ÿç±»å‹: {platform.system()}")
                st.info(f"ğŸ” æ£€æµ‹åˆ°çš„å­—ä½“: {font_path or 'æ— '}")
                return None
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        
        # è®¾ç½®æ ‡é¢˜ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£ç¡®
        try:
            selected_font = font_config.get('selected_font', 'SimHei')
            title_fontsize = max(14, font_config.get('font_size', 12) + 4)
            ax.set_title('è¯äº‘å›¾', fontsize=title_fontsize, pad=20, fontproperties=selected_font)
        except:
            # å¦‚æœç”¨æˆ·é€‰æ‹©çš„å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
            title_fontsize = max(14, font_config.get('font_size', 12) + 4)
            ax.set_title('è¯äº‘å›¾', fontsize=title_fontsize, pad=20)
        
        return fig

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ“ å†…å®¹è¡Œä¸ºåˆ†æ")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
    if 'data_loaded' not in st.session_state or not st.session_state.data_loaded:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¸»é¡µåŠ è½½æ•°æ®")
        st.stop()
    
    # è·å–æ•°æ®
    df = st.session_state.get('filtered_data', st.session_state.current_data)
    
    # ç¡®ä¿æ•°æ®ä¸ä¸ºNone
    if df is None:
        st.error("âŒ æ•°æ®è·å–å¤±è´¥ï¼Œè¯·è¿”å›ä¸»é¡µé‡æ–°åŠ è½½æ•°æ®")
        st.stop()
    
    analyzer = ContentAnalyzer()
    
    # ä¾§è¾¹æ æ§åˆ¶
    st.sidebar.subheader("ğŸ“ åˆ†æé€‰é¡¹")
    
    analysis_type = st.sidebar.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹",
        ["æ–‡æœ¬å†…å®¹åˆ†æ", "å‘å¸ƒæ¥æºåˆ†æ", "å†…å®¹ä¸»é¢˜åˆ†æ", "è¯äº‘åˆ†æ", "ç»¼åˆå†…å®¹æŠ¥å‘Š"]
    )
    
    # é€‰æ‹©æ–‡æœ¬åˆ—
    text_columns = [col for col in df.columns if df[col].dtype == 'object' and ('å†…å®¹' in col or 'æ–‡æœ¬' in col)]
    if not text_columns:
        text_columns = [col for col in df.columns if df[col].dtype == 'object']
    
    if text_columns:
        text_column = st.sidebar.selectbox("é€‰æ‹©æ–‡æœ¬å­—æ®µ", text_columns, index=0)
    else:
        st.error("âŒ æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å­—æ®µ")
        st.stop()
    
    # æ•°æ®æ¦‚è§ˆ
    st.subheader("ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    metrics = create_dashboard_metrics(df)
    display_metrics_cards(metrics)
    
    # æ ¹æ®é€‰æ‹©çš„åˆ†æç±»å‹æ˜¾ç¤ºå†…å®¹
    if analysis_type == "æ–‡æœ¬å†…å®¹åˆ†æ":
        show_text_content_analysis(df, analyzer, text_column)
    elif analysis_type == "å‘å¸ƒæ¥æºåˆ†æ":
        show_posting_source_analysis(df, analyzer)
    elif analysis_type == "å†…å®¹ä¸»é¢˜åˆ†æ":
        show_content_topic_analysis(df, analyzer, text_column)
    elif analysis_type == "è¯äº‘åˆ†æ":
        show_wordcloud_analysis(df, analyzer, text_column)
    elif analysis_type == "ç»¼åˆå†…å®¹æŠ¥å‘Š":
        show_comprehensive_content_report(df, analyzer, text_column)

def show_text_content_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºæ–‡æœ¬å†…å®¹åˆ†æ"""
    st.subheader("ğŸ“Š æ–‡æœ¬å†…å®¹åˆ†æ")
    
    # åˆ†ææ–‡æœ¬å†…å®¹
    content_analysis = analyzer.analyze_text_content(df, text_column)
    
    if not content_analysis:
        st.warning(f"æ— æ³•åˆ†ææ–‡æœ¬å†…å®¹ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    # åŸºç¡€ç»Ÿè®¡
    if 'basic_stats' in content_analysis:
        st.subheader("ğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡")
        
        stats = content_analysis['basic_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»å‘å¸ƒæ•°", stats['total_posts'])
        with col2:
            st.metric("å¹³å‡é•¿åº¦", f"{stats['avg_length']:.1f}å­—")
        with col3:
            st.metric("æœ€å¤§é•¿åº¦", f"{stats['max_length']}å­—")
        with col4:
            st.metric("æœ€å°é•¿åº¦", f"{stats['min_length']}å­—")
        
        # é•¿åº¦åˆ†å¸ƒ
        if 'length_distribution' in content_analysis:
            st.write("**æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ**")
            length_dist = content_analysis['length_distribution']
            
            fig_length = go.Figure(data=[
                go.Bar(
                    x=list(length_dist.keys()),
                    y=list(length_dist.values()),
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(length_dist.values()),
                    textposition='auto'
                )
            ])
            fig_length.update_layout(
                title="æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ",
                xaxis_title="é•¿åº¦ç±»åˆ«",
                yaxis_title="æ•°é‡"
            )
            st.plotly_chart(fig_length, use_container_width=True)
    
    # å…³é”®è¯åˆ†æ
    if 'keywords' in content_analysis:
        st.subheader("ğŸ”‘ å…³é”®è¯åˆ†æ")
        
        keywords = content_analysis['keywords'][:20]  # å–å‰20ä¸ªå…³é”®è¯
        
        if keywords:
            col1, col2 = st.columns(2)
            
            with col1:
                # å…³é”®è¯æƒé‡å›¾
                words, weights = zip(*keywords)
                
                fig_keywords = go.Figure(data=[
                    go.Bar(
                        x=list(weights),
                        y=list(words),
                        orientation='h',
                        marker_color=analyzer.visualizer.color_palette[1],
                        text=[f"{w:.3f}" for w in weights],
                        textposition='auto'
                    )
                ])
                fig_keywords.update_layout(
                    title="å…³é”®è¯æƒé‡æ’è¡Œ",
                    xaxis_title="æƒé‡",
                    yaxis_title="å…³é”®è¯",
                    height=500
                )
                st.plotly_chart(fig_keywords, use_container_width=True)
            
            with col2:
                # å…³é”®è¯è¡¨æ ¼
                st.write("**å…³é”®è¯è¯¦æƒ…**")
                keywords_df = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'æƒé‡'])
                keywords_df['æƒé‡'] = keywords_df['æƒé‡'].round(4)
                st.dataframe(keywords_df, use_container_width=True)
    
    # è¯é¢‘ç»Ÿè®¡
    if 'word_frequency' in content_analysis:
        st.subheader("ğŸ“ˆ é«˜é¢‘è¯æ±‡")
        
        word_freq = content_analysis['word_frequency']
        
        if word_freq:
            # è¯é¢‘æŸ±çŠ¶å›¾
            words = list(word_freq.keys())[:15]  # å–å‰15ä¸ª
            freqs = [word_freq[word] for word in words]
            
            fig_freq = go.Figure(data=[
                go.Bar(
                    x=words,
                    y=freqs,
                    marker_color=analyzer.visualizer.color_palette[2],
                    text=freqs,
                    textposition='auto'
                )
            ])
            fig_freq.update_layout(
                title="é«˜é¢‘è¯æ±‡ç»Ÿè®¡",
                xaxis_title="è¯æ±‡",
                yaxis_title="é¢‘æ¬¡"
            )
            st.plotly_chart(fig_freq, use_container_width=True)
    
    # æƒ…æ„Ÿåˆ†æ
    if 'sentiment_simple' in content_analysis:
        st.subheader("ğŸ˜Š ç®€å•æƒ…æ„Ÿåˆ†æ")
        
        sentiment = content_analysis['sentiment_simple']
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç§¯æè¯æ±‡æåŠ", sentiment['positive_mentions'])
        with col2:
            st.metric("æ¶ˆæè¯æ±‡æåŠ", sentiment['negative_mentions'])
        with col3:
            st.metric("æƒ…æ„Ÿå€¾å‘æ¯”ä¾‹", f"{sentiment['sentiment_ratio']:.2f}")
        
        # æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
        sentiment_data = {
            'ç§¯æ': sentiment['positive_mentions'],
            'æ¶ˆæ': sentiment['negative_mentions'],
            'ä¸­æ€§': max(0, content_analysis['basic_stats']['total_posts'] - sentiment['positive_mentions'] - sentiment['negative_mentions'])
        }
        
        fig_sentiment = go.Figure(data=[
            go.Pie(
                labels=list(sentiment_data.keys()),
                values=list(sentiment_data.values()),
                hole=0.3
            )
        ])
        fig_sentiment.update_layout(title="æƒ…æ„Ÿåˆ†å¸ƒ")
        st.plotly_chart(fig_sentiment, use_container_width=True)
    
    # ç‰¹æ®Šå…ƒç´ åˆ†æ
    if 'special_elements' in content_analysis:
        st.subheader("ğŸ¯ ç‰¹æ®Šå…ƒç´ åˆ†æ")
        
        special = content_analysis['special_elements']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("è¡¨æƒ…ç¬¦å·", special['emoji_count'])
        with col2:
            st.metric("é“¾æ¥æ•°é‡", special['url_count'])
        with col3:
            st.metric("@æåŠ", special['mention_count'])
        with col4:
            st.metric("è¯é¢˜æ ‡ç­¾", special['hashtag_count'])
        
        # ç‰¹æ®Šå…ƒç´ ä½¿ç”¨ç‡
        total_posts = content_analysis['basic_stats']['total_posts']
        usage_rates = {
            'è¡¨æƒ…ç¬¦å·ä½¿ç”¨ç‡': special['posts_with_emoji'] / total_posts * 100,
            'é“¾æ¥ä½¿ç”¨ç‡': special['posts_with_url'] / total_posts * 100,
            '@æåŠä½¿ç”¨ç‡': special['posts_with_mention'] / total_posts * 100,
            'è¯é¢˜æ ‡ç­¾ä½¿ç”¨ç‡': special['posts_with_hashtag'] / total_posts * 100
        }
        
        fig_usage = go.Figure(data=[
            go.Bar(
                x=list(usage_rates.keys()),
                y=list(usage_rates.values()),
                marker_color=analyzer.visualizer.color_palette[3],
                text=[f"{rate:.1f}%" for rate in usage_rates.values()],
                textposition='auto'
            )
        ])
        fig_usage.update_layout(
            title="ç‰¹æ®Šå…ƒç´ ä½¿ç”¨ç‡",
            xaxis_title="å…ƒç´ ç±»å‹",
            yaxis_title="ä½¿ç”¨ç‡(%)"
        )
        st.plotly_chart(fig_usage, use_container_width=True)

def show_posting_source_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer):
    """æ˜¾ç¤ºå‘å¸ƒæ¥æºåˆ†æ"""
    st.subheader("ğŸ“± å‘å¸ƒæ¥æºåˆ†æ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘å¸ƒæ¥æºå­—æ®µ
    source_columns = [col for col in df.columns if 'æ¥æº' in col or 'source' in col.lower()]
    
    if not source_columns:
        st.warning("âŒ æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°å‘å¸ƒæ¥æºå­—æ®µ")
        return
    
    source_column = source_columns[0]
    
    # åˆ†æå‘å¸ƒæ¥æº
    source_analysis = analyzer.analyze_posting_sources(df, source_column)
    
    if not source_analysis:
        st.warning(f"æ— æ³•åˆ†æå‘å¸ƒæ¥æºï¼Œè¯·æ£€æŸ¥{source_column}å­—æ®µ")
        return
    
    # æ¥æºç»Ÿè®¡
    if 'source_stats' in source_analysis:
        st.subheader("ğŸ“Š æ¥æºç»Ÿè®¡æ¦‚è§ˆ")
        
        stats = source_analysis['source_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ¥æºæ€»æ•°", stats['total_sources'])
        with col2:
            st.metric("æœ€ä¸»è¦æ¥æº", stats['most_popular_source'] or "æœªçŸ¥")
        with col3:
            st.metric("ä¸»è¦æ¥æºå æ¯”", f"{stats['most_popular_ratio']*100:.1f}%")
        with col4:
            st.metric("æ¥æºå¤šæ ·æ€§", f"{stats['source_diversity']:.3f}")
    
    # æ¥æºåˆ†å¸ƒ
    if 'source_distribution' in source_analysis:
        st.subheader("ğŸ“ˆ æ¥æºåˆ†å¸ƒ")
        
        source_dist = source_analysis['source_distribution']
        
        # å–å‰10ä¸ªæ¥æº
        top_sources = dict(list(source_dist.items())[:10])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # æ¥æºæŸ±çŠ¶å›¾
            fig_source = go.Figure(data=[
                go.Bar(
                    x=list(top_sources.values()),
                    y=list(top_sources.keys()),
                    orientation='h',
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(top_sources.values()),
                    textposition='auto'
                )
            ])
            fig_source.update_layout(
                title="ä¸»è¦å‘å¸ƒæ¥æº(å‰10)",
                xaxis_title="å‘å¸ƒæ•°é‡",
                yaxis_title="æ¥æº",
                height=400
            )
            st.plotly_chart(fig_source, use_container_width=True)
        
        with col2:
            # æ¥æºé¥¼å›¾
            # åˆå¹¶å°æ¥æº
            other_count = sum(list(source_dist.values())[5:])
            pie_data = dict(list(source_dist.items())[:5])
            if other_count > 0:
                pie_data['å…¶ä»–'] = other_count
            
            fig_pie = go.Figure(data=[
                go.Pie(
                    labels=list(pie_data.keys()),
                    values=list(pie_data.values()),
                    hole=0.3
                )
            ])
            fig_pie.update_layout(title="æ¥æºåˆ†å¸ƒå æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)
    
    # æ¥æºç±»å‹åˆ†æ
    if 'source_categories' in source_analysis:
        st.subheader("ğŸ“± æ¥æºç±»å‹åˆ†æ")
        
        categories = source_analysis['source_categories']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç§»åŠ¨ç«¯", categories['mobile'])
        with col2:
            st.metric("ç½‘é¡µç«¯", categories['web'])
        with col3:
            st.metric("åº”ç”¨ç«¯", categories['app'])
        with col4:
            st.metric("å…¶ä»–", categories['other'])
        
        # ç±»å‹åˆ†å¸ƒå›¾
        category_labels = ['ç§»åŠ¨ç«¯', 'ç½‘é¡µç«¯', 'åº”ç”¨ç«¯', 'å…¶ä»–']
        category_values = [categories['mobile'], categories['web'], categories['app'], categories['other']]
        
        fig_category = go.Figure(data=[
            go.Bar(
                x=category_labels,
                y=category_values,
                marker_color=analyzer.visualizer.color_palette[:4],
                text=category_values,
                textposition='auto'
            )
        ])
        fig_category.update_layout(
            title="å‘å¸ƒæ¥æºç±»å‹åˆ†å¸ƒ",
            xaxis_title="æ¥æºç±»å‹",
            yaxis_title="å‘å¸ƒæ•°é‡"
        )
        st.plotly_chart(fig_category, use_container_width=True)
        
        # ç§»åŠ¨ç«¯å æ¯”åˆ†æ
        total_posts = sum(category_values)
        if total_posts > 0:
            mobile_ratio = categories['mobile'] / total_posts * 100
            if mobile_ratio > 70:
                st.success(f"ğŸ“± ç§»åŠ¨ç«¯å‘å¸ƒå ä¸»å¯¼åœ°ä½({mobile_ratio:.1f}%)")
            elif mobile_ratio > 40:
                st.info(f"ğŸ“Š ç§»åŠ¨ç«¯å’Œå…¶ä»–ç«¯å¹¶é‡({mobile_ratio:.1f}%)")
            else:
                st.warning(f"ğŸ’» éç§»åŠ¨ç«¯å‘å¸ƒè¾ƒå¤š({mobile_ratio:.1f}%)")

def show_content_topic_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºå†…å®¹ä¸»é¢˜åˆ†æ"""
    st.subheader("ğŸ·ï¸ å†…å®¹ä¸»é¢˜åˆ†æ")
    
    # åˆ†æå†…å®¹ä¸»é¢˜
    topic_analysis = analyzer.analyze_content_topics(df, text_column)
    
    if not topic_analysis:
        st.warning(f"æ— æ³•åˆ†æå†…å®¹ä¸»é¢˜ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    # ä¸»é¢˜ç»Ÿè®¡
    if 'topic_stats' in topic_analysis:
        st.subheader("ğŸ“Š ä¸»é¢˜ç»Ÿè®¡æ¦‚è§ˆ")
        
        stats = topic_analysis['topic_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("å·²åˆ†ç±»å†…å®¹", stats['total_classified'])
        with col2:
            st.metric("æœªåˆ†ç±»å†…å®¹", stats['unclassified'])
        with col3:
            st.metric("åˆ†ç±»è¦†ç›–ç‡", f"{stats['classification_rate']*100:.1f}%")
        with col4:
            st.metric("ä¸»é¢˜å¤šæ ·æ€§", stats['topic_diversity'])
        
        if stats['most_popular_topic']:
            st.info(f"ğŸ† æœ€çƒ­é—¨ä¸»é¢˜ï¼š{stats['most_popular_topic']}")
    
    # ä¸»é¢˜åˆ†å¸ƒ
    if 'topic_distribution' in topic_analysis:
        st.subheader("ğŸ“ˆ ä¸»é¢˜åˆ†å¸ƒ")
        
        topic_dist = topic_analysis['topic_distribution']
        
        # è¿‡æ»¤æ‰è®¡æ•°ä¸º0çš„ä¸»é¢˜
        active_topics = {k: v for k, v in topic_dist.items() if v > 0}
        
        if active_topics:
            col1, col2 = st.columns(2)
            
            with col1:
                # ä¸»é¢˜æŸ±çŠ¶å›¾
                fig_topic = go.Figure(data=[
                    go.Bar(
                        x=list(active_topics.keys()),
                        y=list(active_topics.values()),
                        marker_color=analyzer.visualizer.color_palette[0],
                        text=list(active_topics.values()),
                        textposition='auto'
                    )
                ])
                fig_topic.update_layout(
                    title="ä¸»é¢˜åˆ†å¸ƒ",
                    xaxis_title="ä¸»é¢˜",
                    yaxis_title="å†…å®¹æ•°é‡",
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig_topic, use_container_width=True)
            
            with col2:
                # ä¸»é¢˜é¥¼å›¾
                fig_topic_pie = go.Figure(data=[
                    go.Pie(
                        labels=list(active_topics.keys()),
                        values=list(active_topics.values()),
                        hole=0.3
                    )
                ])
                fig_topic_pie.update_layout(title="ä¸»é¢˜å æ¯”åˆ†å¸ƒ")
                st.plotly_chart(fig_topic_pie, use_container_width=True)
            
            # ä¸»é¢˜è¯¦æƒ…è¡¨æ ¼
            st.subheader("ğŸ“‹ ä¸»é¢˜è¯¦æƒ…")
            
            topic_details = []
            total_classified = sum(active_topics.values())
            
            for topic, count in active_topics.items():
                percentage = count / total_classified * 100 if total_classified > 0 else 0
                topic_details.append({
                    'ä¸»é¢˜': topic,
                    'å†…å®¹æ•°é‡': count,
                    'å æ¯”(%)': f"{percentage:.1f}%"
                })
            
            topic_df = pd.DataFrame(topic_details)
            topic_df = topic_df.sort_values('å†…å®¹æ•°é‡', ascending=False)
            st.dataframe(topic_df, use_container_width=True)
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æ˜ç¡®çš„ä¸»é¢˜åˆ†ç±»")

def show_wordcloud_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºè¯äº‘åˆ†æ"""
    st.subheader("â˜ï¸ è¯äº‘åˆ†æ")
    
    # åˆ†ææ–‡æœ¬å†…å®¹è·å–è¯é¢‘
    content_analysis = analyzer.analyze_text_content(df, text_column)
    
    if not content_analysis or 'word_frequency' not in content_analysis:
        st.warning(f"æ— æ³•ç”Ÿæˆè¯äº‘ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    word_freq = content_analysis['word_frequency']
    
    if not word_freq:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„è¯æ±‡æ•°æ®ç”Ÿæˆè¯äº‘")
        return
    
    # è¯äº‘å‚æ•°è®¾ç½®
    st.subheader("âš™ï¸ è¯äº‘è®¾ç½®")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        max_words = st.slider("æœ€å¤§è¯æ±‡æ•°", 20, 200, 100)
    with col2:
        min_freq = st.slider("æœ€å°è¯é¢‘", 1, 10, 2)
    with col3:
        min_word_len = st.slider("æœ€å°è¯é•¿", 2, 5, 2)
    
    # é«˜çº§è¿‡æ»¤é€‰é¡¹
    with st.expander("ğŸ”§ é«˜çº§è¿‡æ»¤é€‰é¡¹", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            filter_numbers = st.checkbox("è¿‡æ»¤çº¯æ•°å­—", value=True)
            filter_english = st.checkbox("è¿‡æ»¤çº¯è‹±æ–‡", value=False)
        with col2:
            filter_single_char = st.checkbox("è¿‡æ»¤å•å­—ç¬¦", value=True)
            custom_stopwords = st.text_area("è‡ªå®šä¹‰åœç”¨è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰", placeholder="ä¾‹å¦‚ï¼šå¾®åš,è½¬å‘,è¯„è®º")
    
    # åº”ç”¨é«˜çº§è¿‡æ»¤é€‰é¡¹
    additional_stopwords = set()
    if custom_stopwords:
        additional_stopwords = set([word.strip() for word in custom_stopwords.split(',') if word.strip()])
    
    # è¿‡æ»¤è¯é¢‘
    filtered_word_freq = {}
    for word, freq in word_freq.items():
        # åŸºç¡€é¢‘ç‡è¿‡æ»¤
        if freq < min_freq:
            continue
        
        # è¯é•¿è¿‡æ»¤
        if len(word) < min_word_len:
            continue
            
        # è‡ªå®šä¹‰åœç”¨è¯è¿‡æ»¤
        if word in additional_stopwords:
            continue
            
        # æ•°å­—è¿‡æ»¤
        if filter_numbers and word.isdigit():
            continue
            
        # è‹±æ–‡è¿‡æ»¤
        if filter_english and word.isascii() and word.isalpha():
            continue
            
        # å•å­—ç¬¦è¿‡æ»¤
        if filter_single_char and len(word) == 1:
            continue
            
        filtered_word_freq[word] = freq
    
    if not filtered_word_freq:
        st.warning(f"åº”ç”¨è¿‡æ»¤æ¡ä»¶åæ²¡æœ‰ç¬¦åˆè¦æ±‚çš„è¯æ±‡")
        return
    
    # ç”Ÿæˆè¯äº‘
    st.subheader("â˜ï¸ è¯äº‘å›¾")
    
    try:
        # ä½¿ç”¨matplotlibç”Ÿæˆè¯äº‘
        wordcloud_fig = analyzer.create_wordcloud(filtered_word_freq, max_words)
        st.pyplot(wordcloud_fig)
        
        # è¯é¢‘ç»Ÿè®¡è¡¨
        st.subheader("ğŸ“Š è¯é¢‘ç»Ÿè®¡")
        
        # æ˜¾ç¤ºå‰20ä¸ªé«˜é¢‘è¯
        top_words = dict(list(filtered_word_freq.items())[:20])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # è¯é¢‘æŸ±çŠ¶å›¾
            fig_freq = go.Figure(data=[
                go.Bar(
                    x=list(top_words.keys()),
                    y=list(top_words.values()),
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(top_words.values()),
                    textposition='auto'
                )
            ])
            fig_freq.update_layout(
                title="é«˜é¢‘è¯æ±‡(å‰20)",
                xaxis_title="è¯æ±‡",
                yaxis_title="é¢‘æ¬¡",
                xaxis_tickangle=-45
            )
            st.plotly_chart(fig_freq, use_container_width=True)
        
        with col2:
            # è¯é¢‘è¡¨æ ¼
            freq_data = []
            for word, freq in list(filtered_word_freq.items())[:20]:
                freq_data.append({'è¯æ±‡': word, 'é¢‘æ¬¡': freq})
            
            freq_df = pd.DataFrame(freq_data)
            st.dataframe(freq_df, use_container_width=True)
        
        # è¯æ±‡æ´å¯Ÿ
        st.subheader("ğŸ’¡ è¯æ±‡æ´å¯Ÿ")
        
        total_words = sum(filtered_word_freq.values())
        unique_words = len(filtered_word_freq)
        most_freq_word = max(filtered_word_freq, key=filtered_word_freq.get)
        most_freq_count = filtered_word_freq[most_freq_word]
        
        insights = [
            f"ğŸ“ å…±è¯†åˆ«å‡º{unique_words}ä¸ªä¸åŒè¯æ±‡ï¼Œæ€»è¯é¢‘{total_words}",
            f"ğŸ† æœ€é«˜é¢‘è¯æ±‡ï¼š'{most_freq_word}'ï¼Œå‡ºç°{most_freq_count}æ¬¡",
            f"ğŸ“Š è¯æ±‡å¤šæ ·æ€§ï¼šå¹³å‡æ¯ä¸ªè¯æ±‡å‡ºç°{total_words/unique_words:.1f}æ¬¡",
            f"ğŸ¯ è¯äº‘å±•ç¤ºäº†ç”¨æˆ·å†…å®¹çš„ä¸»è¦å…³æ³¨ç‚¹å’Œè¯é¢˜å€¾å‘"
        ]
        
        for insight in insights:
            st.info(insight)
    
    except Exception as e:
        st.error(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™ï¼š{str(e)}")
        st.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯å­—ä½“é—®é¢˜ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿä¸­æœ‰ä¸­æ–‡å­—ä½“æ–‡ä»¶")

def show_comprehensive_content_report(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºç»¼åˆå†…å®¹æŠ¥å‘Š"""
    st.subheader("ğŸ“‹ ç»¼åˆå†…å®¹è¡Œä¸ºæŠ¥å‘Š")
    
    # è·å–æ‰€æœ‰åˆ†æç»“æœ
    content_analysis = analyzer.analyze_text_content(df, text_column)
    topic_analysis = analyzer.analyze_content_topics(df, text_column)
    
    # æ£€æŸ¥å‘å¸ƒæ¥æº
    source_columns = [col for col in df.columns if 'æ¥æº' in col or 'source' in col.lower()]
    source_analysis = {}
    if source_columns:
        source_analysis = analyzer.analyze_posting_sources(df, source_columns[0])
    
    # å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
    st.subheader("ğŸ“Š å…³é”®æŒ‡æ ‡æ¦‚è§ˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if content_analysis and 'basic_stats' in content_analysis:
            avg_length = content_analysis['basic_stats']['avg_length']
            st.metric("å¹³å‡æ–‡æœ¬é•¿åº¦", f"{avg_length:.0f}å­—")
    
    with col2:
        if content_analysis and 'special_elements' in content_analysis:
            emoji_rate = content_analysis['special_elements']['avg_emoji_per_post']
            st.metric("å¹³å‡è¡¨æƒ…ä½¿ç”¨", f"{emoji_rate:.1f}ä¸ª/æ¡")
    
    with col3:
        if topic_analysis and 'topic_stats' in topic_analysis:
            classification_rate = topic_analysis['topic_stats']['classification_rate']
            st.metric("ä¸»é¢˜åˆ†ç±»è¦†ç›–ç‡", f"{classification_rate*100:.1f}%")
    
    with col4:
        if source_analysis and 'source_stats' in source_analysis:
            source_diversity = source_analysis['source_stats']['source_diversity']
            st.metric("å‘å¸ƒæ¥æºå¤šæ ·æ€§", f"{source_diversity:.3f}")
    
    # å†…å®¹ç‰¹å¾æ€»ç»“
    st.subheader("ğŸ“ å†…å®¹ç‰¹å¾æ€»ç»“")
    
    content_insights = []
    
    # æ–‡æœ¬é•¿åº¦ç‰¹å¾
    if content_analysis and 'basic_stats' in content_analysis:
        stats = content_analysis['basic_stats']
        avg_length = stats['avg_length']
        
        if avg_length > 200:
            content_insights.append(f"ğŸ“ ç”¨æˆ·å€¾å‘äºå‘å¸ƒé•¿æ–‡æœ¬å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
        elif avg_length > 100:
            content_insights.append(f"ğŸ“„ ç”¨æˆ·å‘å¸ƒä¸­ç­‰é•¿åº¦å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
        else:
            content_insights.append(f"ğŸ“ ç”¨æˆ·å€¾å‘äºå‘å¸ƒç®€çŸ­å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
    
    # æƒ…æ„Ÿç‰¹å¾
    if content_analysis and 'sentiment_simple' in content_analysis:
        sentiment = content_analysis['sentiment_simple']
        sentiment_ratio = sentiment['sentiment_ratio']
        
        if sentiment_ratio > 0.6:
            content_insights.append("ğŸ˜Š å†…å®¹æ•´ä½“æƒ…æ„Ÿå€¾å‘ç§¯ææ­£é¢")
        elif sentiment_ratio > 0.4:
            content_insights.append("ğŸ˜ å†…å®¹æƒ…æ„Ÿå€¾å‘ç›¸å¯¹ä¸­æ€§")
        else:
            content_insights.append("ğŸ˜” å†…å®¹ä¸­æ¶ˆææƒ…æ„Ÿè¡¨è¾¾è¾ƒå¤š")
    
    # ç‰¹æ®Šå…ƒç´ ä½¿ç”¨
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        emoji_usage = special['posts_with_emoji'] / total_posts
        if emoji_usage > 0.5:
            content_insights.append(f"ğŸ˜€ ç”¨æˆ·é¢‘ç¹ä½¿ç”¨è¡¨æƒ…ç¬¦å·({emoji_usage*100:.1f}%çš„å†…å®¹åŒ…å«è¡¨æƒ…)")
        
        url_usage = special['posts_with_url'] / total_posts
        if url_usage > 0.2:
            content_insights.append(f"ğŸ”— ç”¨æˆ·ç»å¸¸åˆ†äº«é“¾æ¥å†…å®¹({url_usage*100:.1f}%çš„å†…å®¹åŒ…å«é“¾æ¥)")
        
        mention_usage = special['posts_with_mention'] / total_posts
        if mention_usage > 0.3:
            content_insights.append(f"ğŸ‘¥ ç”¨æˆ·äº’åŠ¨æ€§è¾ƒå¼º({mention_usage*100:.1f}%çš„å†…å®¹åŒ…å«@æåŠ)")
    
    # ä¸»é¢˜ç‰¹å¾
    if topic_analysis and 'topic_distribution' in topic_analysis:
        topic_dist = topic_analysis['topic_distribution']
        active_topics = {k: v for k, v in topic_dist.items() if v > 0}
        
        if active_topics:
            most_popular_topic = max(active_topics, key=active_topics.get)
            topic_count = active_topics[most_popular_topic]
            total_classified = sum(active_topics.values())
            topic_ratio = topic_count / total_classified if total_classified > 0 else 0
            
            content_insights.append(f"ğŸ·ï¸ ç”¨æˆ·æœ€å…³æ³¨'{most_popular_topic}'ä¸»é¢˜({topic_ratio*100:.1f}%)")
            
            if len(active_topics) > 5:
                content_insights.append(f"ğŸŒˆ ç”¨æˆ·å†…å®¹ä¸»é¢˜å¤šæ ·åŒ–ï¼Œæ¶‰åŠ{len(active_topics)}ä¸ªä¸åŒé¢†åŸŸ")
    
    # å‘å¸ƒæ¥æºç‰¹å¾
    if source_analysis and 'source_categories' in source_analysis:
        categories = source_analysis['source_categories']
        total_posts = sum(categories.values())
        
        if total_posts > 0:
            mobile_ratio = categories['mobile'] / total_posts
            if mobile_ratio > 0.7:
                content_insights.append(f"ğŸ“± ç”¨æˆ·ä¸»è¦é€šè¿‡ç§»åŠ¨è®¾å¤‡å‘å¸ƒå†…å®¹({mobile_ratio*100:.1f}%)")
            elif mobile_ratio > 0.4:
                content_insights.append(f"ğŸ“Š ç”¨æˆ·åœ¨ç§»åŠ¨ç«¯å’Œå…¶ä»–ç«¯å‘å¸ƒå†…å®¹è¾ƒä¸ºå‡è¡¡")
            else:
                content_insights.append(f"ğŸ’» ç”¨æˆ·æ›´å¤šé€šè¿‡éç§»åŠ¨ç«¯å‘å¸ƒå†…å®¹")
    
    for insight in content_insights:
        st.info(insight)
    
    # å†…å®¹è´¨é‡è¯„ä¼°
    st.subheader("â­ å†…å®¹è´¨é‡è¯„ä¼°")
    
    quality_scores = {}
    
    # é•¿åº¦è´¨é‡è¯„åˆ†
    if content_analysis and 'basic_stats' in content_analysis:
        avg_length = content_analysis['basic_stats']['avg_length']
        if 50 <= avg_length <= 300:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 85
        elif avg_length > 300:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 70
        else:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 60
    
    # å¤šæ ·æ€§è¯„åˆ†
    if content_analysis and 'word_frequency' in content_analysis:
        word_freq = content_analysis['word_frequency']
        unique_words = len(word_freq)
        total_words = sum(word_freq.values())
        diversity_ratio = unique_words / total_words if total_words > 0 else 0
        
        if diversity_ratio > 0.3:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 90
        elif diversity_ratio > 0.2:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 75
        else:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 60
    
    # äº’åŠ¨æ€§è¯„åˆ†
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        interaction_elements = (
            special['posts_with_mention'] + 
            special['posts_with_hashtag'] + 
            special['posts_with_emoji']
        ) / (total_posts * 3)  # æ ‡å‡†åŒ–åˆ°0-1
        
        if interaction_elements > 0.4:
            quality_scores['äº’åŠ¨æ€§'] = 85
        elif interaction_elements > 0.2:
            quality_scores['äº’åŠ¨æ€§'] = 70
        else:
            quality_scores['äº’åŠ¨æ€§'] = 55
    
    # ä¸»é¢˜èšç„¦åº¦è¯„åˆ†
    if topic_analysis and 'topic_stats' in topic_analysis:
        classification_rate = topic_analysis['topic_stats']['classification_rate']
        
        if classification_rate > 0.7:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 80
        elif classification_rate > 0.5:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 65
        else:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 50
    
    if quality_scores:
        # è´¨é‡è¯„åˆ†å¯è§†åŒ–
        fig_quality = go.Figure(data=[
            go.Bar(
                x=list(quality_scores.keys()),
                y=list(quality_scores.values()),
                marker_color=analyzer.visualizer.color_palette[0],
                text=[f"{score}åˆ†" for score in quality_scores.values()],
                textposition='auto'
            )
        ])
        fig_quality.update_layout(
            title="å†…å®¹è´¨é‡è¯„ä¼°",
            xaxis_title="è¯„ä¼°ç»´åº¦",
            yaxis_title="å¾—åˆ†",
            yaxis=dict(range=[0, 100])
        )
        st.plotly_chart(fig_quality, use_container_width=True)
        
        # æ€»ä½“è´¨é‡è¯„åˆ†
        overall_score = sum(quality_scores.values()) / len(quality_scores)
        
        if overall_score >= 80:
            st.success(f"ğŸŒŸ å†…å®¹è´¨é‡ä¼˜ç§€ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        elif overall_score >= 70:
            st.info(f"ğŸ‘ å†…å®¹è´¨é‡è‰¯å¥½ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        elif overall_score >= 60:
            st.warning(f"âš ï¸ å†…å®¹è´¨é‡ä¸€èˆ¬ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        else:
            st.error(f"ğŸ“‰ å†…å®¹è´¨é‡éœ€è¦æ”¹è¿›ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
    
    # ä¼˜åŒ–å»ºè®®
    st.subheader("ğŸ’¡ å†…å®¹ä¼˜åŒ–å»ºè®®")
    
    recommendations = []
    
    # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
    if content_analysis and 'basic_stats' in content_analysis:
        avg_length = content_analysis['basic_stats']['avg_length']
        if avg_length < 50:
            recommendations.append("ğŸ“ **å¢åŠ å†…å®¹é•¿åº¦**ï¼šé€‚å½“å¢åŠ å†…å®¹çš„è¯¦ç»†ç¨‹åº¦å’Œæ·±åº¦")
        elif avg_length > 500:
            recommendations.append("âœ‚ï¸ **ç²¾ç®€å†…å®¹**ï¼šè€ƒè™‘å°†é•¿å†…å®¹åˆ†æ®µæˆ–æç‚¼é‡ç‚¹")
    
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        if special['posts_with_emoji'] / total_posts < 0.3:
            recommendations.append("ğŸ˜Š **å¢åŠ è¡¨æƒ…ä½¿ç”¨**ï¼šé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ å†…å®¹äº²å’ŒåŠ›")
        
        if special['posts_with_hashtag'] / total_posts < 0.2:
            recommendations.append("ğŸ·ï¸ **ä½¿ç”¨è¯é¢˜æ ‡ç­¾**ï¼šæ·»åŠ ç›¸å…³è¯é¢˜æ ‡ç­¾æé«˜å†…å®¹å¯å‘ç°æ€§")
    
    if topic_analysis and 'topic_stats' in topic_analysis:
        classification_rate = topic_analysis['topic_stats']['classification_rate']
        if classification_rate < 0.5:
            recommendations.append("ğŸ¯ **æ˜ç¡®å†…å®¹ä¸»é¢˜**ï¼šè®©å†…å®¹ä¸»é¢˜æ›´åŠ æ˜ç¡®å’Œèšç„¦")
    
    # é€šç”¨å»ºè®®
    general_recommendations = [
        "ğŸ“… **å®šæœŸå‘å¸ƒ**ï¼šä¿æŒç¨³å®šçš„å†…å®¹å‘å¸ƒé¢‘ç‡",
        "ğŸ‘¥ **å¢å¼ºäº’åŠ¨**ï¼šå¤šä¸å…¶ä»–ç”¨æˆ·è¿›è¡Œäº’åŠ¨å’Œäº¤æµ",
        "ğŸ“Š **æ•°æ®é©±åŠ¨**ï¼šæ ¹æ®æ•°æ®åˆ†æç»“æœè°ƒæ•´å†…å®¹ç­–ç•¥",
        "ğŸ”„ **æŒç»­ä¼˜åŒ–**ï¼šå®šæœŸå›é¡¾å’Œä¼˜åŒ–å†…å®¹è´¨é‡"
    ]
    
    all_recommendations = recommendations + general_recommendations
    
    for recommendation in all_recommendations:
        st.markdown(recommendation)
    
    # æ•°æ®è¯´æ˜
    st.subheader("â„¹ï¸ åˆ†æè¯´æ˜")
    
    analysis_notes = [
        f"ğŸ“ åˆ†æåŸºäº{len(df)}æ¡å†…å®¹è®°å½•",
        "ğŸ”¤ æ–‡æœ¬åˆ†æé‡‡ç”¨ä¸­æ–‡åˆ†è¯å’Œå…³é”®è¯æå–æŠ€æœ¯",
        "ğŸ·ï¸ ä¸»é¢˜åˆ†ç±»åŸºäºå…³é”®è¯åŒ¹é…æ–¹æ³•",
        "ğŸ˜Š æƒ…æ„Ÿåˆ†æé‡‡ç”¨ç®€å•è¯å…¸åŒ¹é…æ–¹æ³•",
        "âš ï¸ åˆ†æç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…åº”ç”¨éœ€ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯"
    ]
    
    for note in analysis_notes:
        st.caption(note)

if __name__ == "__main__":
    main()