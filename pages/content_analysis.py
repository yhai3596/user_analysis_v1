import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import jieba
import jieba.analyse
from collections import Counter
import re
import sys
from pathlib import Path
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
from textstat import flesch_reading_ease
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils.visualizer import UserBehaviorVisualizer, create_dashboard_metrics, display_metrics_cards
from utils.cache_manager import cache_data
from config.settings import get_config

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å†…å®¹è¡Œä¸ºåˆ†æ",
    page_icon="ğŸ“",
    layout="wide"
)

class ContentAnalyzer:
    """å†…å®¹è¡Œä¸ºåˆ†æå™¨"""
    
    def __init__(self):
        self.visualizer = UserBehaviorVisualizer()
        self.viz_config = get_config('viz')
        
        # åˆå§‹åŒ–jieba
        jieba.initialize()
        
        # åœç”¨è¯åˆ—è¡¨
        self.stop_words = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å“ªä¸ª', 'å¤šå°‘', 'å‡ ä¸ª', 'ç¬¬ä¸€', 'å¯ä»¥', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'å·²ç»', 'è¿˜æ˜¯', 'æˆ–è€…', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'ç°åœ¨', 'ä»¥å', 'ä»¥å‰', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©'
        ])
    
    @cache_data(ttl=1800)
    def analyze_text_content(self, df: pd.DataFrame, text_column: str = 'å¾®åšå†…å®¹') -> dict:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        analysis = {}
        
        if text_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼
        df_text = df[df[text_column].notna()].copy()
        if df_text.empty:
            return analysis
        
        texts = df_text[text_column].astype(str)
        
        # åŸºç¡€ç»Ÿè®¡
        analysis['basic_stats'] = {
            'total_posts': len(texts),
            'avg_length': texts.str.len().mean(),
            'median_length': texts.str.len().median(),
            'max_length': texts.str.len().max(),
            'min_length': texts.str.len().min(),
            'std_length': texts.str.len().std()
        }
        
        # é•¿åº¦åˆ†å¸ƒ
        length_bins = [0, 50, 100, 200, 500, float('inf')]
        length_labels = ['çŸ­æ–‡æœ¬(â‰¤50)', 'ä¸­çŸ­æ–‡æœ¬(51-100)', 'ä¸­ç­‰æ–‡æœ¬(101-200)', 'é•¿æ–‡æœ¬(201-500)', 'è¶…é•¿æ–‡æœ¬(>500)']
        length_distribution = pd.cut(texts.str.len(), bins=length_bins, labels=length_labels, right=False)
        analysis['length_distribution'] = length_distribution.value_counts().to_dict()
        
        # æå–å…³é”®è¯
        all_text = ' '.join(texts)
        keywords = jieba.analyse.extract_tags(all_text, topK=50, withWeight=True)
        analysis['keywords'] = [(word, weight) for word, weight in keywords if word not in self.stop_words]
        
        # è¯é¢‘ç»Ÿè®¡
        words = []
        for text in texts:
            words.extend([word for word in jieba.cut(text) if len(word) > 1 and word not in self.stop_words])
        
        word_freq = Counter(words)
        analysis['word_frequency'] = dict(word_freq.most_common(30))
        
        # æƒ…æ„Ÿè¯åˆ†æï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        positive_words = ['å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'å¼€å¿ƒ', 'å¿«ä¹', 'æ»¡æ„', 'ä¼˜ç§€', 'å®Œç¾', 'ç¾å¥½', 'å¹¸ç¦', 'æˆåŠŸ']
        negative_words = ['å·®', 'å', 'çƒ‚', 'è®¨åŒ', 'éš¾è¿‡', 'å¤±æœ›', 'ç³Ÿç³•', 'ç—›è‹¦', 'å¤±è´¥', 'é—®é¢˜', 'é”™è¯¯']
        
        positive_count = sum(text.count(word) for text in texts for word in positive_words)
        negative_count = sum(text.count(word) for text in texts for word in negative_words)
        
        analysis['sentiment_simple'] = {
            'positive_mentions': positive_count,
            'negative_mentions': negative_count,
            'sentiment_ratio': positive_count / (positive_count + negative_count + 1)
        }
        
        # ç‰¹æ®Šå­—ç¬¦å’Œè¡¨æƒ…åˆ†æ
        emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ğŸ‡€-ğŸ‡¿]+', re.UNICODE)
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        mention_pattern = re.compile(r'@[\w\u4e00-\u9fff]+')
        hashtag_pattern = re.compile(r'#[^#]+#')
        
        emoji_count = sum(len(emoji_pattern.findall(text)) for text in texts)
        url_count = sum(len(url_pattern.findall(text)) for text in texts)
        mention_count = sum(len(mention_pattern.findall(text)) for text in texts)
        hashtag_count = sum(len(hashtag_pattern.findall(text)) for text in texts)
        
        analysis['special_elements'] = {
            'emoji_count': emoji_count,
            'url_count': url_count,
            'mention_count': mention_count,
            'hashtag_count': hashtag_count,
            'avg_emoji_per_post': emoji_count / len(texts),
            'posts_with_emoji': sum(1 for text in texts if emoji_pattern.search(text)),
            'posts_with_url': sum(1 for text in texts if url_pattern.search(text)),
            'posts_with_mention': sum(1 for text in texts if mention_pattern.search(text)),
            'posts_with_hashtag': sum(1 for text in texts if hashtag_pattern.search(text))
        }
        
        return analysis
    
    @cache_data(ttl=1800)
    def analyze_posting_sources(self, df: pd.DataFrame, source_column: str = 'å‘å¸ƒæ¥æº') -> dict:
        """åˆ†æå‘å¸ƒæ¥æº"""
        analysis = {}
        
        if source_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼
        df_source = df[df[source_column].notna()].copy()
        if df_source.empty:
            return analysis
        
        sources = df_source[source_column]
        
        # æ¥æºåˆ†å¸ƒ
        source_counts = sources.value_counts()
        analysis['source_distribution'] = source_counts.to_dict()
        
        # æ¥æºç±»å‹åˆ†ç±»
        mobile_keywords = ['iPhone', 'Android', 'æ‰‹æœº', 'ç§»åŠ¨', 'mobile', 'iOS']
        web_keywords = ['ç½‘é¡µ', 'web', 'æµè§ˆå™¨', 'PC', 'ç”µè„‘']
        app_keywords = ['å®¢æˆ·ç«¯', 'APP', 'åº”ç”¨']
        
        mobile_count = 0
        web_count = 0
        app_count = 0
        other_count = 0
        
        for source in sources:
            source_lower = str(source).lower()
            if any(keyword.lower() in source_lower for keyword in mobile_keywords):
                mobile_count += 1
            elif any(keyword.lower() in source_lower for keyword in web_keywords):
                web_count += 1
            elif any(keyword.lower() in source_lower for keyword in app_keywords):
                app_count += 1
            else:
                other_count += 1
        
        analysis['source_categories'] = {
            'mobile': mobile_count,
            'web': web_count,
            'app': app_count,
            'other': other_count
        }
        
        # ä¸»è¦æ¥æºç»Ÿè®¡
        total_posts = len(sources)
        analysis['source_stats'] = {
            'total_sources': len(source_counts),
            'most_popular_source': source_counts.index[0] if len(source_counts) > 0 else None,
            'most_popular_count': source_counts.iloc[0] if len(source_counts) > 0 else 0,
            'most_popular_ratio': source_counts.iloc[0] / total_posts if len(source_counts) > 0 else 0,
            'source_diversity': len(source_counts) / total_posts  # æ¥æºå¤šæ ·æ€§
        }
        
        return analysis
    
    @cache_data(ttl=1800)
    def analyze_content_topics(self, df: pd.DataFrame, text_column: str = 'å¾®åšå†…å®¹') -> dict:
        """åˆ†æå†…å®¹ä¸»é¢˜"""
        analysis = {}
        
        if text_column not in df.columns:
            return analysis
        
        # è¿‡æ»¤ç©ºå€¼
        df_text = df[df[text_column].notna()].copy()
        if df_text.empty:
            return analysis
        
        texts = df_text[text_column].astype(str)
        
        # ä¸»é¢˜å…³é”®è¯åˆ†ç±»ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        topic_keywords = {
            'ç”Ÿæ´»æ—¥å¸¸': ['ç”Ÿæ´»', 'æ—¥å¸¸', 'åƒé¥­', 'ç¡è§‰', 'å·¥ä½œ', 'å­¦ä¹ ', 'å®¶', 'æœ‹å‹', 'å®¶äºº'],
            'æƒ…æ„Ÿè¡¨è¾¾': ['çˆ±', 'å–œæ¬¢', 'è®¨åŒ', 'å¼€å¿ƒ', 'éš¾è¿‡', 'ç”Ÿæ°”', 'æ„ŸåŠ¨', 'æƒ³å¿µ', 'æ€å¿µ'],
            'å¨±ä¹ä¼‘é—²': ['ç”µå½±', 'éŸ³ä¹', 'æ¸¸æˆ', 'æ—…æ¸¸', 'è´­ç‰©', 'ç¾é£Ÿ', 'è¿åŠ¨', 'å¥èº«'],
            'ç¤¾ä¼šçƒ­ç‚¹': ['æ–°é—»', 'æ”¿æ²»', 'ç»æµ', 'ç¤¾ä¼š', 'çƒ­ç‚¹', 'äº‹ä»¶', 'è®¨è®º', 'è§‚ç‚¹'],
            'ç§‘æŠ€æ•°ç ': ['ç§‘æŠ€', 'æ‰‹æœº', 'ç”µè„‘', 'è½¯ä»¶', 'ç½‘ç»œ', 'äº’è”ç½‘', 'AI', 'æŠ€æœ¯'],
            'å¥åº·å…»ç”Ÿ': ['å¥åº·', 'å…»ç”Ÿ', 'åŒ»ç–—', 'é”»ç‚¼', 'é¥®é£Ÿ', 'è¥å…»', 'ä¿å¥', 'èº«ä½“'],
            'æ•™è‚²å­¦ä¹ ': ['å­¦ä¹ ', 'æ•™è‚²', 'çŸ¥è¯†', 'æŠ€èƒ½', 'è¯¾ç¨‹', 'è€ƒè¯•', 'è¯»ä¹¦', 'æˆé•¿'],
            'èŒåœºå·¥ä½œ': ['å·¥ä½œ', 'èŒåœº', 'åŒäº‹', 'è€æ¿', 'é¡¹ç›®', 'ä¼šè®®', 'åŠ ç­', 'å‡èŒ']
        }
        
        topic_counts = {topic: 0 for topic in topic_keywords.keys()}
        topic_posts = {topic: [] for topic in topic_keywords.keys()}
        
        for idx, text in enumerate(texts):
            text_lower = text.lower()
            for topic, keywords in topic_keywords.items():
                if any(keyword in text_lower for keyword in keywords):
                    topic_counts[topic] += 1
                    topic_posts[topic].append(idx)
        
        analysis['topic_distribution'] = topic_counts
        analysis['topic_posts'] = topic_posts
        
        # ä¸»é¢˜å¤šæ ·æ€§
        total_classified = sum(topic_counts.values())
        unclassified = len(texts) - total_classified
        
        analysis['topic_stats'] = {
            'total_classified': total_classified,
            'unclassified': unclassified,
            'classification_rate': total_classified / len(texts),
            'most_popular_topic': max(topic_counts, key=topic_counts.get) if total_classified > 0 else None,
            'topic_diversity': len([count for count in topic_counts.values() if count > 0])
        }
        
        return analysis
    
    def create_wordcloud(self, word_freq: dict, max_words: int = 100) -> plt.Figure:
        """åˆ›å»ºè¯äº‘å›¾"""
        if not word_freq:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, 'æš‚æ— æ•°æ®', ha='center', va='center', fontsize=20)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            return fig
        
        # åˆ›å»ºè¯äº‘
        try:
            # äº‘ç¯å¢ƒå…¼å®¹çš„WordCloudé…ç½®
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=max_words,
                colormap='viridis',
                prefer_horizontal=0.9,
                relative_scaling=0.5,
                collocations=False,
                mode='RGBA'
            ).generate_from_frequencies(word_freq)
        except Exception as e:
            # å¦‚æœå‡ºç°ä»»ä½•é—®é¢˜ï¼Œä½¿ç”¨æœ€ç®€é…ç½®
            try:
                wordcloud = WordCloud(
                    width=800,
                    height=400,
                    background_color='white',
                    max_words=max_words,
                    mode='RGBA'
                ).generate_from_frequencies(word_freq)
            except Exception as e2:
                st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {str(e2)}")
                st.info("ğŸ’¡ æç¤ºï¼šè¿™å¯èƒ½æ˜¯äº‘ç¯å¢ƒçš„å­—ä½“æˆ–å›¾åƒå¤„ç†é—®é¢˜")
                return None
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('è¯äº‘å›¾', fontsize=16, pad=20)
        
        return fig

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ“ å†…å®¹è¡Œä¸ºåˆ†æ")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²åŠ è½½
    if 'data_loaded' not in st.session_state or not st.session_state.data_loaded:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¸»é¡µåŠ è½½æ•°æ®")
        st.stop()
    
    # è·å–æ•°æ®
    df = st.session_state.get('filtered_data', st.session_state.current_data)
    
    # ç¡®ä¿æ•°æ®ä¸ä¸ºNone
    if df is None:
        st.error("âŒ æ•°æ®è·å–å¤±è´¥ï¼Œè¯·è¿”å›ä¸»é¡µé‡æ–°åŠ è½½æ•°æ®")
        st.stop()
    
    analyzer = ContentAnalyzer()
    
    # ä¾§è¾¹æ æ§åˆ¶
    st.sidebar.subheader("ğŸ“ åˆ†æé€‰é¡¹")
    
    analysis_type = st.sidebar.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹",
        ["æ–‡æœ¬å†…å®¹åˆ†æ", "å‘å¸ƒæ¥æºåˆ†æ", "å†…å®¹ä¸»é¢˜åˆ†æ", "è¯äº‘åˆ†æ", "ç»¼åˆå†…å®¹æŠ¥å‘Š"]
    )
    
    # é€‰æ‹©æ–‡æœ¬åˆ—
    text_columns = [col for col in df.columns if df[col].dtype == 'object' and 'å†…å®¹' in col]
    if not text_columns:
        text_columns = [col for col in df.columns if df[col].dtype == 'object']
    
    if text_columns:
        text_column = st.sidebar.selectbox("é€‰æ‹©æ–‡æœ¬å­—æ®µ", text_columns, index=0)
    else:
        st.error("âŒ æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å­—æ®µ")
        st.stop()
    
    # æ•°æ®æ¦‚è§ˆ
    st.subheader("ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    metrics = create_dashboard_metrics(df)
    display_metrics_cards(metrics)
    
    # æ ¹æ®é€‰æ‹©çš„åˆ†æç±»å‹æ˜¾ç¤ºå†…å®¹
    if analysis_type == "æ–‡æœ¬å†…å®¹åˆ†æ":
        show_text_content_analysis(df, analyzer, text_column)
    elif analysis_type == "å‘å¸ƒæ¥æºåˆ†æ":
        show_posting_source_analysis(df, analyzer)
    elif analysis_type == "å†…å®¹ä¸»é¢˜åˆ†æ":
        show_content_topic_analysis(df, analyzer, text_column)
    elif analysis_type == "è¯äº‘åˆ†æ":
        show_wordcloud_analysis(df, analyzer, text_column)
    elif analysis_type == "ç»¼åˆå†…å®¹æŠ¥å‘Š":
        show_comprehensive_content_report(df, analyzer, text_column)

def show_text_content_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºæ–‡æœ¬å†…å®¹åˆ†æ"""
    st.subheader("ğŸ“Š æ–‡æœ¬å†…å®¹åˆ†æ")
    
    # åˆ†ææ–‡æœ¬å†…å®¹
    content_analysis = analyzer.analyze_text_content(df, text_column)
    
    if not content_analysis:
        st.warning(f"æ— æ³•åˆ†ææ–‡æœ¬å†…å®¹ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    # åŸºç¡€ç»Ÿè®¡
    if 'basic_stats' in content_analysis:
        st.subheader("ğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡")
        
        stats = content_analysis['basic_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»å‘å¸ƒæ•°", stats['total_posts'])
        with col2:
            st.metric("å¹³å‡é•¿åº¦", f"{stats['avg_length']:.1f}å­—")
        with col3:
            st.metric("æœ€å¤§é•¿åº¦", f"{stats['max_length']}å­—")
        with col4:
            st.metric("æœ€å°é•¿åº¦", f"{stats['min_length']}å­—")
        
        # é•¿åº¦åˆ†å¸ƒ
        if 'length_distribution' in content_analysis:
            st.write("**æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ**")
            length_dist = content_analysis['length_distribution']
            
            fig_length = go.Figure(data=[
                go.Bar(
                    x=list(length_dist.keys()),
                    y=list(length_dist.values()),
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(length_dist.values()),
                    textposition='auto'
                )
            ])
            fig_length.update_layout(
                title="æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ",
                xaxis_title="é•¿åº¦ç±»åˆ«",
                yaxis_title="æ•°é‡"
            )
            st.plotly_chart(fig_length, use_container_width=True)
    
    # å…³é”®è¯åˆ†æ
    if 'keywords' in content_analysis:
        st.subheader("ğŸ”‘ å…³é”®è¯åˆ†æ")
        
        keywords = content_analysis['keywords'][:20]  # å–å‰20ä¸ªå…³é”®è¯
        
        if keywords:
            col1, col2 = st.columns(2)
            
            with col1:
                # å…³é”®è¯æƒé‡å›¾
                words, weights = zip(*keywords)
                
                fig_keywords = go.Figure(data=[
                    go.Bar(
                        x=list(weights),
                        y=list(words),
                        orientation='h',
                        marker_color=analyzer.visualizer.color_palette[1],
                        text=[f"{w:.3f}" for w in weights],
                        textposition='auto'
                    )
                ])
                fig_keywords.update_layout(
                    title="å…³é”®è¯æƒé‡æ’è¡Œ",
                    xaxis_title="æƒé‡",
                    yaxis_title="å…³é”®è¯",
                    height=500
                )
                st.plotly_chart(fig_keywords, use_container_width=True)
            
            with col2:
                # å…³é”®è¯è¡¨æ ¼
                st.write("**å…³é”®è¯è¯¦æƒ…**")
                keywords_df = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'æƒé‡'])
                keywords_df['æƒé‡'] = keywords_df['æƒé‡'].round(4)
                st.dataframe(keywords_df, use_container_width=True)
    
    # è¯é¢‘ç»Ÿè®¡
    if 'word_frequency' in content_analysis:
        st.subheader("ğŸ“ˆ é«˜é¢‘è¯æ±‡")
        
        word_freq = content_analysis['word_frequency']
        
        if word_freq:
            # è¯é¢‘æŸ±çŠ¶å›¾
            words = list(word_freq.keys())[:15]  # å–å‰15ä¸ª
            freqs = [word_freq[word] for word in words]
            
            fig_freq = go.Figure(data=[
                go.Bar(
                    x=words,
                    y=freqs,
                    marker_color=analyzer.visualizer.color_palette[2],
                    text=freqs,
                    textposition='auto'
                )
            ])
            fig_freq.update_layout(
                title="é«˜é¢‘è¯æ±‡ç»Ÿè®¡",
                xaxis_title="è¯æ±‡",
                yaxis_title="é¢‘æ¬¡"
            )
            st.plotly_chart(fig_freq, use_container_width=True)
    
    # æƒ…æ„Ÿåˆ†æ
    if 'sentiment_simple' in content_analysis:
        st.subheader("ğŸ˜Š ç®€å•æƒ…æ„Ÿåˆ†æ")
        
        sentiment = content_analysis['sentiment_simple']
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç§¯æè¯æ±‡æåŠ", sentiment['positive_mentions'])
        with col2:
            st.metric("æ¶ˆæè¯æ±‡æåŠ", sentiment['negative_mentions'])
        with col3:
            st.metric("æƒ…æ„Ÿå€¾å‘æ¯”ä¾‹", f"{sentiment['sentiment_ratio']:.2f}")
        
        # æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
        sentiment_data = {
            'ç§¯æ': sentiment['positive_mentions'],
            'æ¶ˆæ': sentiment['negative_mentions'],
            'ä¸­æ€§': max(0, content_analysis['basic_stats']['total_posts'] - sentiment['positive_mentions'] - sentiment['negative_mentions'])
        }
        
        fig_sentiment = go.Figure(data=[
            go.Pie(
                labels=list(sentiment_data.keys()),
                values=list(sentiment_data.values()),
                hole=0.3
            )
        ])
        fig_sentiment.update_layout(title="æƒ…æ„Ÿåˆ†å¸ƒ")
        st.plotly_chart(fig_sentiment, use_container_width=True)
    
    # ç‰¹æ®Šå…ƒç´ åˆ†æ
    if 'special_elements' in content_analysis:
        st.subheader("ğŸ¯ ç‰¹æ®Šå…ƒç´ åˆ†æ")
        
        special = content_analysis['special_elements']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("è¡¨æƒ…ç¬¦å·", special['emoji_count'])
        with col2:
            st.metric("é“¾æ¥æ•°é‡", special['url_count'])
        with col3:
            st.metric("@æåŠ", special['mention_count'])
        with col4:
            st.metric("è¯é¢˜æ ‡ç­¾", special['hashtag_count'])
        
        # ç‰¹æ®Šå…ƒç´ ä½¿ç”¨ç‡
        total_posts = content_analysis['basic_stats']['total_posts']
        usage_rates = {
            'è¡¨æƒ…ç¬¦å·ä½¿ç”¨ç‡': special['posts_with_emoji'] / total_posts * 100,
            'é“¾æ¥ä½¿ç”¨ç‡': special['posts_with_url'] / total_posts * 100,
            '@æåŠä½¿ç”¨ç‡': special['posts_with_mention'] / total_posts * 100,
            'è¯é¢˜æ ‡ç­¾ä½¿ç”¨ç‡': special['posts_with_hashtag'] / total_posts * 100
        }
        
        fig_usage = go.Figure(data=[
            go.Bar(
                x=list(usage_rates.keys()),
                y=list(usage_rates.values()),
                marker_color=analyzer.visualizer.color_palette[3],
                text=[f"{rate:.1f}%" for rate in usage_rates.values()],
                textposition='auto'
            )
        ])
        fig_usage.update_layout(
            title="ç‰¹æ®Šå…ƒç´ ä½¿ç”¨ç‡",
            xaxis_title="å…ƒç´ ç±»å‹",
            yaxis_title="ä½¿ç”¨ç‡(%)"
        )
        st.plotly_chart(fig_usage, use_container_width=True)

def show_posting_source_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer):
    """æ˜¾ç¤ºå‘å¸ƒæ¥æºåˆ†æ"""
    st.subheader("ğŸ“± å‘å¸ƒæ¥æºåˆ†æ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘å¸ƒæ¥æºå­—æ®µ
    source_columns = [col for col in df.columns if 'æ¥æº' in col or 'source' in col.lower()]
    
    if not source_columns:
        st.warning("âŒ æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°å‘å¸ƒæ¥æºå­—æ®µ")
        return
    
    source_column = source_columns[0]
    
    # åˆ†æå‘å¸ƒæ¥æº
    source_analysis = analyzer.analyze_posting_sources(df, source_column)
    
    if not source_analysis:
        st.warning(f"æ— æ³•åˆ†æå‘å¸ƒæ¥æºï¼Œè¯·æ£€æŸ¥{source_column}å­—æ®µ")
        return
    
    # æ¥æºç»Ÿè®¡
    if 'source_stats' in source_analysis:
        st.subheader("ğŸ“Š æ¥æºç»Ÿè®¡æ¦‚è§ˆ")
        
        stats = source_analysis['source_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ¥æºæ€»æ•°", stats['total_sources'])
        with col2:
            st.metric("æœ€ä¸»è¦æ¥æº", stats['most_popular_source'] or "æœªçŸ¥")
        with col3:
            st.metric("ä¸»è¦æ¥æºå æ¯”", f"{stats['most_popular_ratio']*100:.1f}%")
        with col4:
            st.metric("æ¥æºå¤šæ ·æ€§", f"{stats['source_diversity']:.3f}")
    
    # æ¥æºåˆ†å¸ƒ
    if 'source_distribution' in source_analysis:
        st.subheader("ğŸ“ˆ æ¥æºåˆ†å¸ƒ")
        
        source_dist = source_analysis['source_distribution']
        
        # å–å‰10ä¸ªæ¥æº
        top_sources = dict(list(source_dist.items())[:10])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # æ¥æºæŸ±çŠ¶å›¾
            fig_source = go.Figure(data=[
                go.Bar(
                    x=list(top_sources.values()),
                    y=list(top_sources.keys()),
                    orientation='h',
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(top_sources.values()),
                    textposition='auto'
                )
            ])
            fig_source.update_layout(
                title="ä¸»è¦å‘å¸ƒæ¥æº(å‰10)",
                xaxis_title="å‘å¸ƒæ•°é‡",
                yaxis_title="æ¥æº",
                height=400
            )
            st.plotly_chart(fig_source, use_container_width=True)
        
        with col2:
            # æ¥æºé¥¼å›¾
            # åˆå¹¶å°æ¥æº
            other_count = sum(list(source_dist.values())[5:])
            pie_data = dict(list(source_dist.items())[:5])
            if other_count > 0:
                pie_data['å…¶ä»–'] = other_count
            
            fig_pie = go.Figure(data=[
                go.Pie(
                    labels=list(pie_data.keys()),
                    values=list(pie_data.values()),
                    hole=0.3
                )
            ])
            fig_pie.update_layout(title="æ¥æºåˆ†å¸ƒå æ¯”")
            st.plotly_chart(fig_pie, use_container_width=True)
    
    # æ¥æºç±»å‹åˆ†æ
    if 'source_categories' in source_analysis:
        st.subheader("ğŸ“± æ¥æºç±»å‹åˆ†æ")
        
        categories = source_analysis['source_categories']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç§»åŠ¨ç«¯", categories['mobile'])
        with col2:
            st.metric("ç½‘é¡µç«¯", categories['web'])
        with col3:
            st.metric("åº”ç”¨ç«¯", categories['app'])
        with col4:
            st.metric("å…¶ä»–", categories['other'])
        
        # ç±»å‹åˆ†å¸ƒå›¾
        category_labels = ['ç§»åŠ¨ç«¯', 'ç½‘é¡µç«¯', 'åº”ç”¨ç«¯', 'å…¶ä»–']
        category_values = [categories['mobile'], categories['web'], categories['app'], categories['other']]
        
        fig_category = go.Figure(data=[
            go.Bar(
                x=category_labels,
                y=category_values,
                marker_color=analyzer.visualizer.color_palette[:4],
                text=category_values,
                textposition='auto'
            )
        ])
        fig_category.update_layout(
            title="å‘å¸ƒæ¥æºç±»å‹åˆ†å¸ƒ",
            xaxis_title="æ¥æºç±»å‹",
            yaxis_title="å‘å¸ƒæ•°é‡"
        )
        st.plotly_chart(fig_category, use_container_width=True)
        
        # ç§»åŠ¨ç«¯å æ¯”åˆ†æ
        total_posts = sum(category_values)
        if total_posts > 0:
            mobile_ratio = categories['mobile'] / total_posts * 100
            if mobile_ratio > 70:
                st.success(f"ğŸ“± ç§»åŠ¨ç«¯å‘å¸ƒå ä¸»å¯¼åœ°ä½({mobile_ratio:.1f}%)")
            elif mobile_ratio > 40:
                st.info(f"ğŸ“Š ç§»åŠ¨ç«¯å’Œå…¶ä»–ç«¯å¹¶é‡({mobile_ratio:.1f}%)")
            else:
                st.warning(f"ğŸ’» éç§»åŠ¨ç«¯å‘å¸ƒè¾ƒå¤š({mobile_ratio:.1f}%)")

def show_content_topic_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºå†…å®¹ä¸»é¢˜åˆ†æ"""
    st.subheader("ğŸ·ï¸ å†…å®¹ä¸»é¢˜åˆ†æ")
    
    # åˆ†æå†…å®¹ä¸»é¢˜
    topic_analysis = analyzer.analyze_content_topics(df, text_column)
    
    if not topic_analysis:
        st.warning(f"æ— æ³•åˆ†æå†…å®¹ä¸»é¢˜ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    # ä¸»é¢˜ç»Ÿè®¡
    if 'topic_stats' in topic_analysis:
        st.subheader("ğŸ“Š ä¸»é¢˜ç»Ÿè®¡æ¦‚è§ˆ")
        
        stats = topic_analysis['topic_stats']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("å·²åˆ†ç±»å†…å®¹", stats['total_classified'])
        with col2:
            st.metric("æœªåˆ†ç±»å†…å®¹", stats['unclassified'])
        with col3:
            st.metric("åˆ†ç±»è¦†ç›–ç‡", f"{stats['classification_rate']*100:.1f}%")
        with col4:
            st.metric("ä¸»é¢˜å¤šæ ·æ€§", stats['topic_diversity'])
        
        if stats['most_popular_topic']:
            st.info(f"ğŸ† æœ€çƒ­é—¨ä¸»é¢˜ï¼š{stats['most_popular_topic']}")
    
    # ä¸»é¢˜åˆ†å¸ƒ
    if 'topic_distribution' in topic_analysis:
        st.subheader("ğŸ“ˆ ä¸»é¢˜åˆ†å¸ƒ")
        
        topic_dist = topic_analysis['topic_distribution']
        
        # è¿‡æ»¤æ‰è®¡æ•°ä¸º0çš„ä¸»é¢˜
        active_topics = {k: v for k, v in topic_dist.items() if v > 0}
        
        if active_topics:
            col1, col2 = st.columns(2)
            
            with col1:
                # ä¸»é¢˜æŸ±çŠ¶å›¾
                fig_topic = go.Figure(data=[
                    go.Bar(
                        x=list(active_topics.keys()),
                        y=list(active_topics.values()),
                        marker_color=analyzer.visualizer.color_palette[0],
                        text=list(active_topics.values()),
                        textposition='auto'
                    )
                ])
                fig_topic.update_layout(
                    title="ä¸»é¢˜åˆ†å¸ƒ",
                    xaxis_title="ä¸»é¢˜",
                    yaxis_title="å†…å®¹æ•°é‡",
                    xaxis_tickangle=-45
                )
                st.plotly_chart(fig_topic, use_container_width=True)
            
            with col2:
                # ä¸»é¢˜é¥¼å›¾
                fig_topic_pie = go.Figure(data=[
                    go.Pie(
                        labels=list(active_topics.keys()),
                        values=list(active_topics.values()),
                        hole=0.3
                    )
                ])
                fig_topic_pie.update_layout(title="ä¸»é¢˜å æ¯”åˆ†å¸ƒ")
                st.plotly_chart(fig_topic_pie, use_container_width=True)
            
            # ä¸»é¢˜è¯¦æƒ…è¡¨æ ¼
            st.subheader("ğŸ“‹ ä¸»é¢˜è¯¦æƒ…")
            
            topic_details = []
            total_classified = sum(active_topics.values())
            
            for topic, count in active_topics.items():
                percentage = count / total_classified * 100 if total_classified > 0 else 0
                topic_details.append({
                    'ä¸»é¢˜': topic,
                    'å†…å®¹æ•°é‡': count,
                    'å æ¯”(%)': f"{percentage:.1f}%"
                })
            
            topic_df = pd.DataFrame(topic_details)
            topic_df = topic_df.sort_values('å†…å®¹æ•°é‡', ascending=False)
            st.dataframe(topic_df, use_container_width=True)
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æ˜ç¡®çš„ä¸»é¢˜åˆ†ç±»")

def show_wordcloud_analysis(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºè¯äº‘åˆ†æ"""
    st.subheader("â˜ï¸ è¯äº‘åˆ†æ")
    
    # åˆ†ææ–‡æœ¬å†…å®¹è·å–è¯é¢‘
    content_analysis = analyzer.analyze_text_content(df, text_column)
    
    if not content_analysis or 'word_frequency' not in content_analysis:
        st.warning(f"æ— æ³•ç”Ÿæˆè¯äº‘ï¼Œè¯·æ£€æŸ¥{text_column}å­—æ®µ")
        return
    
    word_freq = content_analysis['word_frequency']
    
    if not word_freq:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„è¯æ±‡æ•°æ®ç”Ÿæˆè¯äº‘")
        return
    
    # è¯äº‘å‚æ•°è®¾ç½®
    st.subheader("âš™ï¸ è¯äº‘è®¾ç½®")
    
    col1, col2 = st.columns(2)
    with col1:
        max_words = st.slider("æœ€å¤§è¯æ±‡æ•°", 20, 200, 100)
    with col2:
        min_freq = st.slider("æœ€å°è¯é¢‘", 1, 10, 2)
    
    # è¿‡æ»¤è¯é¢‘
    filtered_word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_freq}
    
    if not filtered_word_freq:
        st.warning(f"æ²¡æœ‰è¯é¢‘å¤§äºç­‰äº{min_freq}çš„è¯æ±‡")
        return
    
    # ç”Ÿæˆè¯äº‘
    st.subheader("â˜ï¸ è¯äº‘å›¾")
    
    try:
        # ä½¿ç”¨matplotlibç”Ÿæˆè¯äº‘
        wordcloud_fig = analyzer.create_wordcloud(filtered_word_freq, max_words)
        st.pyplot(wordcloud_fig)
        
        # è¯é¢‘ç»Ÿè®¡è¡¨
        st.subheader("ğŸ“Š è¯é¢‘ç»Ÿè®¡")
        
        # æ˜¾ç¤ºå‰20ä¸ªé«˜é¢‘è¯
        top_words = dict(list(filtered_word_freq.items())[:20])
        
        col1, col2 = st.columns(2)
        
        with col1:
            # è¯é¢‘æŸ±çŠ¶å›¾
            fig_freq = go.Figure(data=[
                go.Bar(
                    x=list(top_words.keys()),
                    y=list(top_words.values()),
                    marker_color=analyzer.visualizer.color_palette[0],
                    text=list(top_words.values()),
                    textposition='auto'
                )
            ])
            fig_freq.update_layout(
                title="é«˜é¢‘è¯æ±‡(å‰20)",
                xaxis_title="è¯æ±‡",
                yaxis_title="é¢‘æ¬¡",
                xaxis_tickangle=-45
            )
            st.plotly_chart(fig_freq, use_container_width=True)
        
        with col2:
            # è¯é¢‘è¡¨æ ¼
            freq_data = []
            for word, freq in list(filtered_word_freq.items())[:20]:
                freq_data.append({'è¯æ±‡': word, 'é¢‘æ¬¡': freq})
            
            freq_df = pd.DataFrame(freq_data)
            st.dataframe(freq_df, use_container_width=True)
        
        # è¯æ±‡æ´å¯Ÿ
        st.subheader("ğŸ’¡ è¯æ±‡æ´å¯Ÿ")
        
        total_words = sum(filtered_word_freq.values())
        unique_words = len(filtered_word_freq)
        most_freq_word = max(filtered_word_freq, key=filtered_word_freq.get)
        most_freq_count = filtered_word_freq[most_freq_word]
        
        insights = [
            f"ğŸ“ å…±è¯†åˆ«å‡º{unique_words}ä¸ªä¸åŒè¯æ±‡ï¼Œæ€»è¯é¢‘{total_words}",
            f"ğŸ† æœ€é«˜é¢‘è¯æ±‡ï¼š'{most_freq_word}'ï¼Œå‡ºç°{most_freq_count}æ¬¡",
            f"ğŸ“Š è¯æ±‡å¤šæ ·æ€§ï¼šå¹³å‡æ¯ä¸ªè¯æ±‡å‡ºç°{total_words/unique_words:.1f}æ¬¡",
            f"ğŸ¯ è¯äº‘å±•ç¤ºäº†ç”¨æˆ·å†…å®¹çš„ä¸»è¦å…³æ³¨ç‚¹å’Œè¯é¢˜å€¾å‘"
        ]
        
        for insight in insights:
            st.info(insight)
    
    except Exception as e:
        st.error(f"ç”Ÿæˆè¯äº‘æ—¶å‡ºé”™ï¼š{str(e)}")
        st.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯å­—ä½“é—®é¢˜ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿä¸­æœ‰ä¸­æ–‡å­—ä½“æ–‡ä»¶")

def show_comprehensive_content_report(df: pd.DataFrame, analyzer: ContentAnalyzer, text_column: str):
    """æ˜¾ç¤ºç»¼åˆå†…å®¹æŠ¥å‘Š"""
    st.subheader("ğŸ“‹ ç»¼åˆå†…å®¹è¡Œä¸ºæŠ¥å‘Š")
    
    # è·å–æ‰€æœ‰åˆ†æç»“æœ
    content_analysis = analyzer.analyze_text_content(df, text_column)
    topic_analysis = analyzer.analyze_content_topics(df, text_column)
    
    # æ£€æŸ¥å‘å¸ƒæ¥æº
    source_columns = [col for col in df.columns if 'æ¥æº' in col or 'source' in col.lower()]
    source_analysis = {}
    if source_columns:
        source_analysis = analyzer.analyze_posting_sources(df, source_columns[0])
    
    # å…³é”®æŒ‡æ ‡æ¦‚è§ˆ
    st.subheader("ğŸ“Š å…³é”®æŒ‡æ ‡æ¦‚è§ˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if content_analysis and 'basic_stats' in content_analysis:
            avg_length = content_analysis['basic_stats']['avg_length']
            st.metric("å¹³å‡æ–‡æœ¬é•¿åº¦", f"{avg_length:.0f}å­—")
    
    with col2:
        if content_analysis and 'special_elements' in content_analysis:
            emoji_rate = content_analysis['special_elements']['avg_emoji_per_post']
            st.metric("å¹³å‡è¡¨æƒ…ä½¿ç”¨", f"{emoji_rate:.1f}ä¸ª/æ¡")
    
    with col3:
        if topic_analysis and 'topic_stats' in topic_analysis:
            classification_rate = topic_analysis['topic_stats']['classification_rate']
            st.metric("ä¸»é¢˜åˆ†ç±»è¦†ç›–ç‡", f"{classification_rate*100:.1f}%")
    
    with col4:
        if source_analysis and 'source_stats' in source_analysis:
            source_diversity = source_analysis['source_stats']['source_diversity']
            st.metric("å‘å¸ƒæ¥æºå¤šæ ·æ€§", f"{source_diversity:.3f}")
    
    # å†…å®¹ç‰¹å¾æ€»ç»“
    st.subheader("ğŸ“ å†…å®¹ç‰¹å¾æ€»ç»“")
    
    content_insights = []
    
    # æ–‡æœ¬é•¿åº¦ç‰¹å¾
    if content_analysis and 'basic_stats' in content_analysis:
        stats = content_analysis['basic_stats']
        avg_length = stats['avg_length']
        
        if avg_length > 200:
            content_insights.append(f"ğŸ“ ç”¨æˆ·å€¾å‘äºå‘å¸ƒé•¿æ–‡æœ¬å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
        elif avg_length > 100:
            content_insights.append(f"ğŸ“„ ç”¨æˆ·å‘å¸ƒä¸­ç­‰é•¿åº¦å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
        else:
            content_insights.append(f"ğŸ“ ç”¨æˆ·å€¾å‘äºå‘å¸ƒç®€çŸ­å†…å®¹ï¼Œå¹³å‡{avg_length:.0f}å­—")
    
    # æƒ…æ„Ÿç‰¹å¾
    if content_analysis and 'sentiment_simple' in content_analysis:
        sentiment = content_analysis['sentiment_simple']
        sentiment_ratio = sentiment['sentiment_ratio']
        
        if sentiment_ratio > 0.6:
            content_insights.append("ğŸ˜Š å†…å®¹æ•´ä½“æƒ…æ„Ÿå€¾å‘ç§¯ææ­£é¢")
        elif sentiment_ratio > 0.4:
            content_insights.append("ğŸ˜ å†…å®¹æƒ…æ„Ÿå€¾å‘ç›¸å¯¹ä¸­æ€§")
        else:
            content_insights.append("ğŸ˜” å†…å®¹ä¸­æ¶ˆææƒ…æ„Ÿè¡¨è¾¾è¾ƒå¤š")
    
    # ç‰¹æ®Šå…ƒç´ ä½¿ç”¨
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        emoji_usage = special['posts_with_emoji'] / total_posts
        if emoji_usage > 0.5:
            content_insights.append(f"ğŸ˜€ ç”¨æˆ·é¢‘ç¹ä½¿ç”¨è¡¨æƒ…ç¬¦å·({emoji_usage*100:.1f}%çš„å†…å®¹åŒ…å«è¡¨æƒ…)")
        
        url_usage = special['posts_with_url'] / total_posts
        if url_usage > 0.2:
            content_insights.append(f"ğŸ”— ç”¨æˆ·ç»å¸¸åˆ†äº«é“¾æ¥å†…å®¹({url_usage*100:.1f}%çš„å†…å®¹åŒ…å«é“¾æ¥)")
        
        mention_usage = special['posts_with_mention'] / total_posts
        if mention_usage > 0.3:
            content_insights.append(f"ğŸ‘¥ ç”¨æˆ·äº’åŠ¨æ€§è¾ƒå¼º({mention_usage*100:.1f}%çš„å†…å®¹åŒ…å«@æåŠ)")
    
    # ä¸»é¢˜ç‰¹å¾
    if topic_analysis and 'topic_distribution' in topic_analysis:
        topic_dist = topic_analysis['topic_distribution']
        active_topics = {k: v for k, v in topic_dist.items() if v > 0}
        
        if active_topics:
            most_popular_topic = max(active_topics, key=active_topics.get)
            topic_count = active_topics[most_popular_topic]
            total_classified = sum(active_topics.values())
            topic_ratio = topic_count / total_classified if total_classified > 0 else 0
            
            content_insights.append(f"ğŸ·ï¸ ç”¨æˆ·æœ€å…³æ³¨'{most_popular_topic}'ä¸»é¢˜({topic_ratio*100:.1f}%)")
            
            if len(active_topics) > 5:
                content_insights.append(f"ğŸŒˆ ç”¨æˆ·å†…å®¹ä¸»é¢˜å¤šæ ·åŒ–ï¼Œæ¶‰åŠ{len(active_topics)}ä¸ªä¸åŒé¢†åŸŸ")
    
    # å‘å¸ƒæ¥æºç‰¹å¾
    if source_analysis and 'source_categories' in source_analysis:
        categories = source_analysis['source_categories']
        total_posts = sum(categories.values())
        
        if total_posts > 0:
            mobile_ratio = categories['mobile'] / total_posts
            if mobile_ratio > 0.7:
                content_insights.append(f"ğŸ“± ç”¨æˆ·ä¸»è¦é€šè¿‡ç§»åŠ¨è®¾å¤‡å‘å¸ƒå†…å®¹({mobile_ratio*100:.1f}%)")
            elif mobile_ratio > 0.4:
                content_insights.append(f"ğŸ“Š ç”¨æˆ·åœ¨ç§»åŠ¨ç«¯å’Œå…¶ä»–ç«¯å‘å¸ƒå†…å®¹è¾ƒä¸ºå‡è¡¡")
            else:
                content_insights.append(f"ğŸ’» ç”¨æˆ·æ›´å¤šé€šè¿‡éç§»åŠ¨ç«¯å‘å¸ƒå†…å®¹")
    
    for insight in content_insights:
        st.info(insight)
    
    # å†…å®¹è´¨é‡è¯„ä¼°
    st.subheader("â­ å†…å®¹è´¨é‡è¯„ä¼°")
    
    quality_scores = {}
    
    # é•¿åº¦è´¨é‡è¯„åˆ†
    if content_analysis and 'basic_stats' in content_analysis:
        avg_length = content_analysis['basic_stats']['avg_length']
        if 50 <= avg_length <= 300:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 85
        elif avg_length > 300:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 70
        else:
            quality_scores['é•¿åº¦é€‚ä¸­'] = 60
    
    # å¤šæ ·æ€§è¯„åˆ†
    if content_analysis and 'word_frequency' in content_analysis:
        word_freq = content_analysis['word_frequency']
        unique_words = len(word_freq)
        total_words = sum(word_freq.values())
        diversity_ratio = unique_words / total_words if total_words > 0 else 0
        
        if diversity_ratio > 0.3:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 90
        elif diversity_ratio > 0.2:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 75
        else:
            quality_scores['è¯æ±‡å¤šæ ·æ€§'] = 60
    
    # äº’åŠ¨æ€§è¯„åˆ†
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        interaction_elements = (
            special['posts_with_mention'] + 
            special['posts_with_hashtag'] + 
            special['posts_with_emoji']
        ) / (total_posts * 3)  # æ ‡å‡†åŒ–åˆ°0-1
        
        if interaction_elements > 0.4:
            quality_scores['äº’åŠ¨æ€§'] = 85
        elif interaction_elements > 0.2:
            quality_scores['äº’åŠ¨æ€§'] = 70
        else:
            quality_scores['äº’åŠ¨æ€§'] = 55
    
    # ä¸»é¢˜èšç„¦åº¦è¯„åˆ†
    if topic_analysis and 'topic_stats' in topic_analysis:
        classification_rate = topic_analysis['topic_stats']['classification_rate']
        
        if classification_rate > 0.7:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 80
        elif classification_rate > 0.5:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 65
        else:
            quality_scores['ä¸»é¢˜èšç„¦åº¦'] = 50
    
    if quality_scores:
        # è´¨é‡è¯„åˆ†å¯è§†åŒ–
        fig_quality = go.Figure(data=[
            go.Bar(
                x=list(quality_scores.keys()),
                y=list(quality_scores.values()),
                marker_color=analyzer.visualizer.color_palette[0],
                text=[f"{score}åˆ†" for score in quality_scores.values()],
                textposition='auto'
            )
        ])
        fig_quality.update_layout(
            title="å†…å®¹è´¨é‡è¯„ä¼°",
            xaxis_title="è¯„ä¼°ç»´åº¦",
            yaxis_title="å¾—åˆ†",
            yaxis=dict(range=[0, 100])
        )
        st.plotly_chart(fig_quality, use_container_width=True)
        
        # æ€»ä½“è´¨é‡è¯„åˆ†
        overall_score = sum(quality_scores.values()) / len(quality_scores)
        
        if overall_score >= 80:
            st.success(f"ğŸŒŸ å†…å®¹è´¨é‡ä¼˜ç§€ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        elif overall_score >= 70:
            st.info(f"ğŸ‘ å†…å®¹è´¨é‡è‰¯å¥½ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        elif overall_score >= 60:
            st.warning(f"âš ï¸ å†…å®¹è´¨é‡ä¸€èˆ¬ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
        else:
            st.error(f"ğŸ“‰ å†…å®¹è´¨é‡éœ€è¦æ”¹è¿›ï¼Œæ€»ä½“å¾—åˆ†ï¼š{overall_score:.1f}åˆ†")
    
    # ä¼˜åŒ–å»ºè®®
    st.subheader("ğŸ’¡ å†…å®¹ä¼˜åŒ–å»ºè®®")
    
    recommendations = []
    
    # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
    if content_analysis and 'basic_stats' in content_analysis:
        avg_length = content_analysis['basic_stats']['avg_length']
        if avg_length < 50:
            recommendations.append("ğŸ“ **å¢åŠ å†…å®¹é•¿åº¦**ï¼šé€‚å½“å¢åŠ å†…å®¹çš„è¯¦ç»†ç¨‹åº¦å’Œæ·±åº¦")
        elif avg_length > 500:
            recommendations.append("âœ‚ï¸ **ç²¾ç®€å†…å®¹**ï¼šè€ƒè™‘å°†é•¿å†…å®¹åˆ†æ®µæˆ–æç‚¼é‡ç‚¹")
    
    if content_analysis and 'special_elements' in content_analysis:
        special = content_analysis['special_elements']
        total_posts = content_analysis['basic_stats']['total_posts']
        
        if special['posts_with_emoji'] / total_posts < 0.3:
            recommendations.append("ğŸ˜Š **å¢åŠ è¡¨æƒ…ä½¿ç”¨**ï¼šé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ å†…å®¹äº²å’ŒåŠ›")
        
        if special['posts_with_hashtag'] / total_posts < 0.2:
            recommendations.append("ğŸ·ï¸ **ä½¿ç”¨è¯é¢˜æ ‡ç­¾**ï¼šæ·»åŠ ç›¸å…³è¯é¢˜æ ‡ç­¾æé«˜å†…å®¹å¯å‘ç°æ€§")
    
    if topic_analysis and 'topic_stats' in topic_analysis:
        classification_rate = topic_analysis['topic_stats']['classification_rate']
        if classification_rate < 0.5:
            recommendations.append("ğŸ¯ **æ˜ç¡®å†…å®¹ä¸»é¢˜**ï¼šè®©å†…å®¹ä¸»é¢˜æ›´åŠ æ˜ç¡®å’Œèšç„¦")
    
    # é€šç”¨å»ºè®®
    general_recommendations = [
        "ğŸ“… **å®šæœŸå‘å¸ƒ**ï¼šä¿æŒç¨³å®šçš„å†…å®¹å‘å¸ƒé¢‘ç‡",
        "ğŸ‘¥ **å¢å¼ºäº’åŠ¨**ï¼šå¤šä¸å…¶ä»–ç”¨æˆ·è¿›è¡Œäº’åŠ¨å’Œäº¤æµ",
        "ğŸ“Š **æ•°æ®é©±åŠ¨**ï¼šæ ¹æ®æ•°æ®åˆ†æç»“æœè°ƒæ•´å†…å®¹ç­–ç•¥",
        "ğŸ”„ **æŒç»­ä¼˜åŒ–**ï¼šå®šæœŸå›é¡¾å’Œä¼˜åŒ–å†…å®¹è´¨é‡"
    ]
    
    all_recommendations = recommendations + general_recommendations
    
    for recommendation in all_recommendations:
        st.markdown(recommendation)
    
    # æ•°æ®è¯´æ˜
    st.subheader("â„¹ï¸ åˆ†æè¯´æ˜")
    
    analysis_notes = [
        f"ğŸ“ åˆ†æåŸºäº{len(df)}æ¡å†…å®¹è®°å½•",
        "ğŸ”¤ æ–‡æœ¬åˆ†æé‡‡ç”¨ä¸­æ–‡åˆ†è¯å’Œå…³é”®è¯æå–æŠ€æœ¯",
        "ğŸ·ï¸ ä¸»é¢˜åˆ†ç±»åŸºäºå…³é”®è¯åŒ¹é…æ–¹æ³•",
        "ğŸ˜Š æƒ…æ„Ÿåˆ†æé‡‡ç”¨ç®€å•è¯å…¸åŒ¹é…æ–¹æ³•",
        "âš ï¸ åˆ†æç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…åº”ç”¨éœ€ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯"
    ]
    
    for note in analysis_notes:
        st.caption(note)

if __name__ == "__main__":
    main()